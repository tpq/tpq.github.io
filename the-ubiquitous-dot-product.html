<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>The Ubiquitous Dot Product</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<link rel="icon" type="image/x-icon" href="favicon.ico" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #0000ff; } /* Keyword */
code > span.ch { color: #008080; } /* Char */
code > span.st { color: #008080; } /* String */
code > span.co { color: #008000; } /* Comment */
code > span.ot { color: #ff4000; } /* Other */
code > span.al { color: #ff0000; } /* Alert */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #008000; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #008080; } /* SpecialChar */
code > span.vs { color: #008080; } /* VerbatimString */
code > span.ss { color: #008080; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #0000ff; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ff4000; } /* Preprocessor */
code > span.do { color: #008000; } /* Documentation */
code > span.an { color: #008000; } /* Annotation */
code > span.cv { color: #008000; } /* CommentVar */
code > span.at { } /* Attribute */
code > span.in { color: #008000; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">THOM QUINN</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about-me.html">ABOUT ME</a>
</li>
<li>
  <a href="curriculum-vitae.html">CURRICULUM VITAE</a>
</li>
<li>
  <a href="musings.html">MUSINGS</a>
</li>
<li>
  <a href="fiction.html">FICTION</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">The Ubiquitous Dot Product</h1>

</div>


<div id="the-dot-in-common" class="section level3">
<h3>The dot in-common</h3>
<p>Through my years of training in biomedicine, and despite taking a number of mathematics and statistics courses, I have somehow missed out on formal education in linear algebra. I feel that this has put me at a disadvantage in the age of next-generation sequencing where linear algebra plays an important role in analyzing large multi-dimensional vectors of genetic data. As part of catching up on a missed opportunity, I wanted to share what I have learned about the dot product and its ubiquity in statistical computing. In doing so, I hope the reader and writer can gain a better intuition of this fascinating operation.</p>
</div>
<div id="sum-definitions" class="section level3">
<h3>Sum definitions</h3>
<p>Wikipedia offers a number of definitions for the dot product. Let us look at the algebraic definition:</p>
<p><span class="math display">\[\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^na_ib_i = a_1b_1+a_2b_2+\dots+a_nb_n\]</span></p>
<p>The dot product operates on two vectors to sum the element-wise product of those vectors. This appears as a simple call in the natively vectorized R programming language.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)
b &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)
<span class="kw">sum</span>(a <span class="op">*</span><span class="st"> </span>b)</code></pre></div>
<pre><code>## [1] -6.367611</code></pre>
<p>Alternatively, we could use the base R operator for the dot product.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a <span class="op">%*%</span><span class="st"> </span>b</code></pre></div>
<pre><code>##           [,1]
## [1,] -6.367611</code></pre>
<p>Below, we will see how the dot product appears in many of the statistical methods we use routinely. But what does the dot product signify? Gaining an intuition for this operation is considerably difficult. Several introductory websites provide a number of geometric explanations. For example, the <a href="https://brilliant.org/wiki/dot-product-distance-between-point-and-a-line/">Brilliant</a> website provides a helpful interactive graph to illustrate how changing the direction or magnitude of two-dimensional vectors can change the dot product of those vectors.</p>
<p>Otherwise, I find it helpful to think of the dot product as a weighted measure of the “agreement” between the two vectors in their joint departure from zero. Consider the case where the absolute value of every element in the vector <span class="math inline">\(\mathbf{b}\)</span> is less than or equal to the corresponding element in <span class="math inline">\(\mathbf{a}\)</span>: if <span class="math inline">\(\mathbf{b}\)</span> approaches <span class="math inline">\(\mathbf{a}\)</span>, then <span class="math inline">\(\mathbf{a} \cdot \mathbf{b}\)</span> will approach the sum of the squares of <span class="math inline">\(\mathbf{a}\)</span>. On the other hand, if <span class="math inline">\(\mathbf{b}\)</span> approaches the opposite of <span class="math inline">\(\mathbf{a}\)</span>, then <span class="math inline">\(\mathbf{a} \cdot \mathbf{b}\)</span> will approach the negative sum of the squares of <span class="math inline">\(\mathbf{a}\)</span>.</p>
</div>
<div id="euclidean-distance" class="section level3">
<h3>Euclidean distance</h3>
<p>We will first take a look at Euclidean distance between two vectors, calculated as the square root of the element-wise differences squared. Let us look at the algebraic definition:</p>
<p><span class="math display">\[d(\mathbf{a}, \mathbf{b}) = \sqrt{\sum_{i=1}^n(a_i - b_i)^2} = \sqrt{\sum_{i=1}^n(a_i^2 + b_i^2 - 2a_ib_i)}\]</span></p>
<p>In the second formulation, we see that if we break up the summation into three separate parts (i.e., <span class="math inline">\(\sum_ia_i^2\)</span>, <span class="math inline">\(\sum_ib_i^2\)</span>, and <span class="math inline">\(\sum_i2a_ib_i\)</span>), we can rewrite Euclidean distance quite neatly using the dot product:</p>
<p><span class="math display">\[d(\mathbf{a}, \mathbf{b}) = \sqrt{\mathbf{a} \cdot \mathbf{a} + \mathbf{b} \cdot \mathbf{b} - 2\mathbf{a}\cdot\mathbf{b}}\]</span></p>
<p>We can calculate Euclidean distance between two vectors in R using the <code>dist</code> function. We compare this to the distance computed using the dot product.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dist</span>(<span class="kw">t</span>(<span class="kw">data.frame</span>(a, b)))</code></pre></div>
<pre><code>##          a
## b 14.97394</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(a <span class="op">%*%</span><span class="st"> </span>a <span class="op">+</span><span class="st"> </span>b <span class="op">%*%</span><span class="st"> </span>b <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>a <span class="op">%*%</span><span class="st"> </span>b)</code></pre></div>
<pre><code>##          [,1]
## [1,] 14.97394</code></pre>
</div>
<div id="variance" class="section level3">
<h3>Variance</h3>
<p>Variance, or standard deviation squared, is fundamental to statistics, making up the foundation of hypothesis testing. Let us look at the algebraic definition:</p>
<p><span class="math display">\[\sigma^2 = \frac{\sum_i(a_i-\mu)^2}{N}\]</span></p>
<p>Variance requires knowledge about two constants: the mean (<span class="math inline">\(\mu\)</span>) and vector length (<span class="math inline">\(N\)</span>). The mean used here is the sum of the elements divided by the total number of elements. When calculating variance, we substract this constant from each element in the vector. In a way, we can think of this as “scaling” the original vector (i.e., <span class="math inline">\(\mathbf{a}_S = \mathbf{a} - \mu_\mathbf{a}\)</span>) so that the new mean equals zero.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">mean</span>(a <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(a)))</code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>From here, we can describe variance using the dot product:</p>
<p><span class="math display">\[\sigma^2 = \frac{\mathbf{a}_S \cdot \mathbf{a}_S}{N} = \frac{(\mathbf{a} - \mu_\mathbf{a}) \cdot (\mathbf{a} - \mu_\mathbf{a})}{N}\]</span></p>
<p>We often calculate the dot product of a vector with itself. We can use the notation <span class="math inline">\(|\mathbf{a}|^2 = \mathbf{a} \cdot \mathbf{a}\)</span> (called a <em>norm</em>) to simplify this routine procedure. This allows us to define variance succintly:</p>
<p><span class="math display">\[\sigma^2 = \frac{|\mathbf{a} - \mu_\mathbf{a}|^2}{N}\]</span></p>
<p>We can calculate variance in R using the <code>var</code> function. We compare this to the variance computed using the dot product. Note that, by default, the <code>var</code> function in R computes <em>sample variance</em>. This means it uses <span class="math inline">\(N - 1\)</span> instead of <span class="math inline">\(N\)</span> as the denominator.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(a)</code></pre></div>
<pre><code>## [1] 0.9248466</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(a <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(a)) <span class="op">%*%</span><span class="st"> </span>(a <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(a)) <span class="op">/</span><span class="st"> </span>(<span class="kw">length</span>(a) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</code></pre></div>
<pre><code>##           [,1]
## [1,] 0.9248466</code></pre>
</div>
<div id="covariance" class="section level3">
<h3>Covariance</h3>
<p>Covariance is an extension of variance involving two vectors. Let us look at the algebraic definition:</p>
<p><span class="math display">\[\textrm{cov}(\mathbf{a}, \mathbf{b})= \frac{\sum_i(a_i-\mu_\mathbf{a})(b_i - \mu_\mathbf{b})}{N}\]</span></p>
<p>Conceiving of <span class="math inline">\(\mathbf{a} - \mu_\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b} - \mu_\mathbf{b}\)</span> as “scaled” vectors, we can rewrite covariance as a dot product:</p>
<p><span class="math display">\[\textrm{cov}(\mathbf{a}, \mathbf{b})= \frac{(\mathbf{a} - \mu_\mathbf{a}) \cdot (\mathbf{b} - \mu_\mathbf{b})}{N}\]</span></p>
<p>We can calculate covariance in R using the <code>cov</code> function. We compare this to the covariance computed using the dot product. Note that, like <code>var</code>, the <code>cov</code> function in R computes <em>sample covariance</em> with <span class="math inline">\(N-1\)</span> as the denominator.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cov</span>(a, b)</code></pre></div>
<pre><code>## [1] -0.05653278</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(a <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(a)) <span class="op">%*%</span><span class="st"> </span>(b <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(b)) <span class="op">/</span><span class="st"> </span>(<span class="kw">length</span>(a) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</code></pre></div>
<pre><code>##             [,1]
## [1,] -0.05653278</code></pre>
</div>
<div id="correlation" class="section level3">
<h3>Correlation</h3>
<p>Pearson’s correlation coefficient is really just a modification to covariance that “scales” it by the product of the individual variances. As such, we can substitute the equation below to define Pearson’s correlation coefficient in terms of the dot product:</p>
<p><span class="math display">\[\rho(\mathbf{a}, \mathbf{b}) = \frac{\textrm{cov}(\mathbf{a}, \mathbf{b})}{\sigma_\mathbf{a}\sigma_\mathbf{b}}\]</span></p>
</div>
<div id="logical-intersections" class="section level3">
<h3>Logical intersections</h3>
<p>Interestingly, if we use <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> to denote binary vectors, then we can use the dot product to count the frequency of the logical intersections between the vectors. For example, given a set with as many as <span class="math inline">\(N\)</span> elements (where the vector <span class="math inline">\(\mathbf{a}\)</span> indicates whether the i-th element belongs to set <span class="math inline">\(\mathbf{A}\)</span>), we can use the dot product to tabulate the intersection of two sets <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>:</p>
<p><span class="math display">\[|\mathbf{A} \cap \mathbf{B}| = \sum_{i=1}^na_i \land b_i = \mathbf{a} \cdot \mathbf{b}\]</span></p>
<p>We can calculate this in R by computing on a boolean vector. We compare this to using the dot product.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">FALSE</span>, <span class="ot">TRUE</span>), <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)
b &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">FALSE</span>, <span class="ot">TRUE</span>), <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)
<span class="kw">sum</span>(a <span class="op">&amp;</span><span class="st"> </span>b)</code></pre></div>
<pre><code>## [1] 22</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a <span class="op">%*%</span><span class="st"> </span>b</code></pre></div>
<pre><code>##      [,1]
## [1,]   22</code></pre>
<p>This is equivalent to the tabulation provided by the <code>table</code> function in R. To use the dot product to tally the other frequencies from this table, we just repeat the calculation on the logically negated vectors (i.e., <span class="math inline">\(!\mathbf{a}\)</span> or <span class="math inline">\(!\mathbf{b}\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(a, b)</code></pre></div>
<pre><code>##        b
## a       FALSE TRUE
##   FALSE    29   32
##   TRUE     17   22</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TT &lt;-<span class="st"> </span>a <span class="op">%*%</span><span class="st"> </span>b
TF &lt;-<span class="st"> </span>a <span class="op">%*%</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>b)
FT &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>a) <span class="op">%*%</span><span class="st"> </span>b
FF &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>a) <span class="op">%*%</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>b)
mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(FF, TF, FT, TT), <span class="dt">nrow =</span> <span class="dv">2</span>)
mat</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   29   32
## [2,]   17   22</code></pre>
<p>This contingency table (analogous to a confusion matrix) serves as the basis for the Fisher’s Exact Test as well as the <span class="math inline">\(\chi^2\)</span> Test. Stated broadly, these test for a significant association between two (or more) presumedly independent sets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fisher.test</span>(mat)</code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  mat
## p-value = 0.8373
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.4850262 2.8537128
## sample estimates:
## odds ratio 
##   1.170924</code></pre>
</div>
<div id="logical-unions" class="section level3">
<h3>Logical unions</h3>
<p>We can also use the dot product to tabulate the frequency of the logical unions between two vectors:</p>
<p><span class="math display">\[|\mathbf{A} \cup \mathbf{B}| = \sum_{i=1}^na_i \lor b_i = \mathbf{a} \cdot \mathbf{a} + \mathbf{b} \cdot \mathbf{b} - \mathbf{a} \cdot \mathbf{b}\]</span></p>
<p>We can calculate this in R by computing on a boolean vector. We compare this to using the dot product.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(a <span class="op">|</span><span class="st"> </span>b)</code></pre></div>
<pre><code>## [1] 71</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a <span class="op">%*%</span><span class="st"> </span>a <span class="op">+</span><span class="st"> </span>b <span class="op">%*%</span><span class="st"> </span>b <span class="op">-</span><span class="st"> </span>a <span class="op">%*%</span><span class="st"> </span>b</code></pre></div>
<pre><code>##      [,1]
## [1,]   71</code></pre>
</div>
<div id="overlap-coefficient" class="section level3">
<h3>Overlap coefficient</h3>
<p>The dot product also appears in the lesser known, but still important, overlap coefficient. This metric measures the degree to which two sets overlap. The overlap coefficient equation plays a role in network analysis where each “set” represents the connectivity of one element (called a <em>node</em>) to all other elements. This provides a way to quantify the amount of common “links” between any two nodes in a network (Ravasz 2002).</p>
<p>The overlap coefficient is useful when analyzing biological networks because distinct biological features (e.g., genes) might have similar functional roles if they share a large number of overlapping partners (even if two do not interact directly). A high overlap coefficient means that the two nodes belong to the same “neighborhood”, regardless of whether the nodes are “neighbors” themselves (Yip 2007). For two binary vectors, the overlap coefficient equals the frequency of the logical intersections as “scaled” by the maximum possible number of intersections:</p>
<p><span class="math display">\[\textrm{overlap}(\mathbf{A},\mathbf{B}) = \frac{|\mathbf{A} \cap \mathbf{B}|}{\textrm{min}(|\mathbf{A}|,|\mathbf{B}|)}\]</span></p>
<p>Here, the notation <span class="math inline">\(|\mathbf{A}| = \sqrt{\mathbf{A} \cdot \mathbf{A}}\)</span> (yet another <em>norm</em>) denotes the sum of the absolute value of the elements in the vector.</p>
</div>
<div id="inner-product" class="section level3">
<h3>Inner product</h3>
<p>Finally, I want to mention that although we discussed the dot product as it pertains to vectors, this operation applies to matrices as well. The dot product of two matrices, called the <strong>inner product</strong>, is defined as the dot product of the i-th <strong>row</strong> vector of the first matrix and the j-th <strong>column</strong> vector of the second matrix (for each row of the first matrix and each column of the second matrix). This means that, unlike the vector dot product, the matrix inner product is not commutative: <span class="math inline">\(\mathbf{A}\cdot\mathbf{B}\neq\mathbf{B}\cdot\mathbf{A}\)</span>. Its definition also requires the first matrix to have the same number of columns as the second has rows.</p>
<p>Given a matrix <span class="math inline">\(\mathbf{A}\)</span> with <span class="math inline">\(m\)</span> columns and a matrix <span class="math inline">\(\mathbf{B}\)</span> with <span class="math inline">\(m\)</span> rows, the i,j-th result of the inner product equals the dot product of the i-th row of <span class="math inline">\(\mathbf{A}\)</span> and the j-th column of <span class="math inline">\(\mathbf{B}\)</span>:</p>
<p><span class="math display">\[(\mathbf{A}\mathbf{B})_{i,j} = \mathbf{A}_{i,} \cdot \mathbf{B}_{, j}\]</span></p>
<p>This might look less cryptic in R code. Note that we use the same <code>%*%</code> operator for matrix multiplication.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5</span><span class="op">^</span><span class="dv">2</span>), <span class="dv">5</span>, <span class="dv">5</span>)
B &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5</span><span class="op">^</span><span class="dv">2</span>), <span class="dv">5</span>, <span class="dv">5</span>)
(A <span class="op">%*%</span><span class="st"> </span>B)[<span class="dv">1</span>, <span class="dv">2</span>]</code></pre></div>
<pre><code>## [1] -0.05027494</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A[<span class="dv">1</span>, ] <span class="op">%*%</span><span class="st"> </span>B[, <span class="dv">2</span>]</code></pre></div>
<pre><code>##             [,1]
## [1,] -0.05027494</code></pre>
<p>As an example, we will show how to use the inner product to repeat a calculation across the combination of every column vector. Specifically, we will calculate all covariances for a matrix. This is akin to what is achieved by the <code>cov</code> function in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cov</span>(A)</code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  3.0528358  2.3816630 -1.2354287  0.3243693  0.5110176
## [2,]  2.3816630  2.1267862 -0.5191054  0.2825757  0.2067718
## [3,] -1.2354287 -0.5191054  1.3062685 -0.1832324 -0.7521320
## [4,]  0.3243693  0.2825757 -0.1832324  0.2055409  0.2846195
## [5,]  0.5110176  0.2067718 -0.7521320  0.2846195  1.1981719</code></pre>
<p>To calculate covariance, we first need to “scale” each column by the column mean. We can do this using the <code>apply</code> function. Next, since the inner product computes <strong>rows</strong> by <strong>columns</strong>, and we want to compute <strong>columns</strong> by <strong>columns</strong>, we need to transpose the first matrix (i.e., in order to have the rows contain column data). Then, we use the <code>%*%</code> operator to compute the covariance matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">As &lt;-<span class="st"> </span><span class="kw">apply</span>(A, <span class="dv">2</span>, <span class="cf">function</span>(x) x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x))
<span class="kw">t</span>(As) <span class="op">%*%</span><span class="st"> </span>As <span class="op">/</span><span class="st"> </span>(<span class="kw">nrow</span>(As) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  3.0528358  2.3816630 -1.2354287  0.3243693  0.5110176
## [2,]  2.3816630  2.1267862 -0.5191054  0.2825757  0.2067718
## [3,] -1.2354287 -0.5191054  1.3062685 -0.1832324 -0.7521320
## [4,]  0.3243693  0.2825757 -0.1832324  0.2055409  0.2846195
## [5,]  0.5110176  0.2067718 -0.7521320  0.2846195  1.1981719</code></pre>
<p>Note that using the dot product in R is much faster than nested <code>for</code> loops. In fact, the performance gain from the dot product can hold true even for low level languages like C++ if using a highly optimized linear algebra library (e.g., see <code>RcppEigen</code>). In cases where a specific function like <code>cov</code> is not available, dot products can make code run faster and look neater. Nevertheless, thinking in terms of the dot product offers a useful way to unify seemingly disparate statistical concepts.</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ol style="list-style-type: decimal">
<li><p>Ravasz, E., A. L. Somera, D. A. Mongru, Z. N. Oltvai, and A.-L. Barabási. “Hierarchical Organization of Modularity in Metabolic Networks.” Science 297, no. 5586 (August 30, 2002): 1551–55. <a href="http://dx.doi.org/10.1126/science.1073374" class="uri">http://dx.doi.org/10.1126/science.1073374</a>.</p></li>
<li><p>Yip, Andy M., and Steve Horvath. “Gene Network Interconnectedness and the Generalized Topological Overlap Measure.” BMC Bioinformatics 8 (2007): 22. <a href="http://dx.doi.org/10.1186/1471-2105-8-22" class="uri">http://dx.doi.org/10.1186/1471-2105-8-22</a>.</p></li>
</ol>
<p><br></p>
</div>

<div style="position: relative">
  <p style="position: relative; bottom: 0; width:100%; text-align: center; font-size: small">
    &copy; 2018 | <a href="mailto:thom@tpq.me">thom@tpq.me</a> |
    <a href="http://twitter.com/tpq__">Twitter</a> |
    <a href="http://github.com/tpq"> GitHub</a>
  </p>
</div>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
