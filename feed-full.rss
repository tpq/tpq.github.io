<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<atom:link rel="self" href="http://ftr.fivefilters.org/makefulltextfeed.php?url=tpq.me%2Ffeed.rss&amp;links=preserve" />
<atom:link rel="alternate" title="Source URL" href="http://tpq.me/feed.rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2Fftr.fivefilters.org%2Fmakefulltextfeed.php%3Furl%3Dtpq.me%252Ffeed.rss%26links%3Dpreserve&amp;back=http%3A%2F%2Fftr.fivefilters.org%2Fmakefulltextfeed.php%3Furl%3Dtpq.me%252Ffeed.rss%26links%3Dpreserve" />
<title>tpq.me</title>
<link>http://www.tpq.me</link>
<description>Personal website of Thom Quinn</description>
<item>
<title>The Ubiquitous Dot Product</title>
<link>http://www.tpq.me/the-ubiquitous-dot-product.html</link>
<guid isPermaLink="true" >http://www.tpq.me/the-ubiquitous-dot-product.html</guid>
<description>&lt;!-- tabsets --&gt;&lt;!-- code folding --&gt;&lt;!--/.navbar --&gt;&lt;div id=&quot;the-dot-in-common&quot; class=&quot;section level3&quot; readability=&quot;14&quot;&gt;&lt;h3&gt;The dot in-common&lt;/h3&gt;&lt;p&gt;Through my years of training in biomedicine, and despite taking a number of mathematics and statistics courses, I have somehow missed out on formal education in linear algebra. I feel that this has put me at a disadvantage in the age of next-generation sequencing where linear algebra plays an important role in analyzing large multi-dimensional vectors of genetic data. As part of catching up on a missed opportunity, I wanted to share what I have learned about the dot product and its ubiquity in statistical computing. In doing so, I hope the reader and writer can gain a better intuition of this fascinating operation.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;sum-definitions&quot; class=&quot;section level3&quot; readability=&quot;36.79682733374&quot;&gt;&lt;h3&gt;Sum definitions&lt;/h3&gt;&lt;p&gt;Wikipedia offers a number of definitions for the dot product. Let us look at the algebraic definition:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^na_ib_i = a_1b_1+a_2b_2+\dots+a_nb_n\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;The dot product operates on two vectors to sum the element-wise product of those vectors. This appears as a simple call in the natively vectorized R programming language.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;a &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
b &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(a *&lt;span class=&quot;st&quot;/&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] -6.367611&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Alternatively, we could use the base R operator for the dot product.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] -6.367611&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Below, we will see how the dot product appears in many of the statistical methods we use routinely. But what does the dot product signify? Gaining an intuition for this operation is considerably difficult. Several introductory websites provide a number of geometric explanations. For example, the &lt;a href=&quot;https://brilliant.org/wiki/dot-product-distance-between-point-and-a-line/&quot;&gt;Brilliant&lt;/a&gt; website provides a helpful interactive graph to illustrate how changing the direction or magnitude of two-dimensional vectors can change the dot product of those vectors.&lt;/p&gt;&lt;p&gt;Otherwise, I find it helpful to think of the dot product as a weighted measure of the “agreement” between the two vectors in their joint departure from zero. Consider the case where the absolute value of every element in the vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; is less than or equal to the corresponding element in &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;: if &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; approaches &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;, then &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a} \cdot \mathbf{b}\)&lt;/span&gt; will approach the sum of the squares of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;. On the other hand, if &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; approaches the opposite of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;, then &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a} \cdot \mathbf{b}\)&lt;/span&gt; will approach the negative sum of the squares of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;euclidean-distance&quot; class=&quot;section level3&quot; readability=&quot;31.5&quot;&gt;&lt;h3&gt;Euclidean distance&lt;/h3&gt;&lt;p&gt;We will first take a look at Euclidean distance between two vectors, calculated as the square root of the element-wise differences squared. Let us look at the algebraic definition:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[d(\mathbf{a}, \mathbf{b}) = \sqrt{\sum_{i=1}^n(a_i - b_i)^2} = \sqrt{\sum_{i=1}^n(a_i^2 + b_i^2 - 2a_ib_i)}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In the second formulation, we see that if we break up the summation into three separate parts (i.e., &lt;span class=&quot;math inline&quot;&gt;\(\sum_ia_i^2\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\sum_ib_i^2\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(\sum_i2a_ib_i\)&lt;/span&gt;), we can rewrite Euclidean distance quite neatly using the dot product:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[d(\mathbf{a}, \mathbf{b}) = \sqrt{\mathbf{a} \cdot \mathbf{a} + \mathbf{b} \cdot \mathbf{b} - 2\mathbf{a}\cdot\mathbf{b}}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We can calculate Euclidean distance between two vectors in R using the &lt;code&gt;dist&lt;/code&gt; function. We compare this to the distance computed using the dot product.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;8&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;dist&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;t&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;data.frame&lt;/span&gt;(a, b)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##          a
## b 14.97394&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sqrt&lt;/span&gt;(a %*%&lt;span class=&quot;st&quot;/&gt;a +&lt;span class=&quot;st&quot;/&gt;b %*%&lt;span class=&quot;st&quot;/&gt;b -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt; *&lt;span class=&quot;st&quot;/&gt;a %*%&lt;span class=&quot;st&quot;/&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##          [,1]
## [1,] 14.97394&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;variance&quot; class=&quot;section level3&quot; readability=&quot;44&quot;&gt;&lt;h3&gt;Variance&lt;/h3&gt;&lt;p&gt;Variance, or standard deviation squared, is fundamental to statistics, making up the foundation of hypothesis testing. Let us look at the algebraic definition:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\sigma^2 = \frac{\sum_i(a_i-\mu)^2}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Variance requires knowledge about two constants: the mean (&lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;) and vector length (&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;). The mean used here is the sum of the elements divided by the total number of elements. When calculating variance, we substract this constant from each element in the vector. In a way, we can think of this as “scaling” the original vector (i.e., &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}_S = \mathbf{a} - \mu_\mathbf{a}\)&lt;/span&gt;) so that the new mean equals zero.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;round&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;From here, we can describe variance using the dot product:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\sigma^2 = \frac{\mathbf{a}_S \cdot \mathbf{a}_S}{N} = \frac{(\mathbf{a} - \mu_\mathbf{a}) \cdot (\mathbf{a} - \mu_\mathbf{a})}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We often calculate the dot product of a vector with itself. We can use the notation &lt;span class=&quot;math inline&quot;&gt;\(|\mathbf{a}|^2 = \mathbf{a} \cdot \mathbf{a}\)&lt;/span&gt; (called a &lt;em&gt;norm&lt;/em&gt;) to simplify this routine procedure. This allows us to define variance succintly:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\sigma^2 = \frac{|\mathbf{a} - \mu_\mathbf{a}|^2}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We can calculate variance in R using the &lt;code&gt;var&lt;/code&gt; function. We compare this to the variance computed using the dot product. Note that, by default, the &lt;code&gt;var&lt;/code&gt; function in R computes &lt;em&gt;sample variance&lt;/em&gt;. This means it uses &lt;span class=&quot;math inline&quot;&gt;\(N - 1\)&lt;/span&gt; instead of &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; as the denominator.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;## [1] 0.9248466&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;(a -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)) %*%&lt;span class=&quot;st&quot;/&gt;(a -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)) /&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(a) -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] 0.9248466&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;covariance&quot; class=&quot;section level3&quot; readability=&quot;31&quot;&gt;&lt;h3&gt;Covariance&lt;/h3&gt;&lt;p&gt;Covariance is an extension of variance involving two vectors. Let us look at the algebraic definition:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\textrm{cov}(\mathbf{a}, \mathbf{b})= \frac{\sum_i(a_i-\mu_\mathbf{a})(b_i - \mu_\mathbf{b})}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Conceiving of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a} - \mu_\mathbf{a}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b} - \mu_\mathbf{b}\)&lt;/span&gt; as “scaled” vectors, we can rewrite covariance as a dot product:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\textrm{cov}(\mathbf{a}, \mathbf{b})= \frac{(\mathbf{a} - \mu_\mathbf{a}) \cdot (\mathbf{b} - \mu_\mathbf{b})}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We can calculate covariance in R using the &lt;code&gt;cov&lt;/code&gt; function. We compare this to the covariance computed using the dot product. Note that, like &lt;code&gt;var&lt;/code&gt;, the &lt;code&gt;cov&lt;/code&gt; function in R computes &lt;em&gt;sample covariance&lt;/em&gt; with &lt;span class=&quot;math inline&quot;&gt;\(N-1\)&lt;/span&gt; as the denominator.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;## [1] -0.05653278&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;(a -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)) %*%&lt;span class=&quot;st&quot;/&gt;(b -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(b)) /&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(a) -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,] -0.05653278&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;correlation&quot; class=&quot;section level3&quot; readability=&quot;15&quot;&gt;&lt;h3&gt;Correlation&lt;/h3&gt;&lt;p&gt;Pearson’s correlation coefficient is really just a modification to covariance that “scales” it by the product of the individual variances. As such, we can substitute the equation below to define Pearson’s correlation coefficient in terms of the dot product:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\rho(\mathbf{a}, \mathbf{b}) = \frac{\textrm{cov}(\mathbf{a}, \mathbf{b})}{\sigma_\mathbf{a}\sigma_\mathbf{b}}\]&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;logical-intersections&quot; class=&quot;section level3&quot; readability=&quot;50&quot;&gt;&lt;h3&gt;Logical intersections&lt;/h3&gt;&lt;p&gt;Interestingly, if we use &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; to denote binary vectors, then we can use the dot product to count the frequency of the logical intersections between the vectors. For example, given a set with as many as &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; elements (where the vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt; indicates whether the i-th element belongs to set &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt;), we can use the dot product to tabulate the intersection of two sets &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{B}\)&lt;/span&gt;:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[|\mathbf{A} \cap \mathbf{B}| = \sum_{i=1}^na_i \land b_i = \mathbf{a} \cdot \mathbf{b}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We can calculate this in R by computing on a boolean vector. We compare this to using the dot product.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;14&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;a &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sample&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;, &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;replace =&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
b &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sample&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;, &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;replace =&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(a &amp;amp;&lt;span class=&quot;st&quot;/&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 22&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,]   22&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is equivalent to the tabulation provided by the &lt;code&gt;table&lt;/code&gt; function in R. To use the dot product to tally the other frequencies from this table, we just repeat the calculation on the logically negated vectors (i.e., &lt;span class=&quot;math inline&quot;&gt;\(!\mathbf{a}\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(!\mathbf{b}\)&lt;/span&gt;).&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;table&lt;/span&gt;(a, b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##        b
## a       FALSE TRUE
##   FALSE    29   32
##   TRUE     17   22&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;TT &amp;lt;-&lt;span class=&quot;st&quot;/&gt;a %*%&lt;span class=&quot;st&quot;/&gt;b
TF &amp;lt;-&lt;span class=&quot;st&quot;/&gt;a %*%&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; -&lt;span class=&quot;st&quot;/&gt;b)
FT &amp;lt;-&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; -&lt;span class=&quot;st&quot;/&gt;a) %*%&lt;span class=&quot;st&quot;/&gt;b
FF &amp;lt;-&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; -&lt;span class=&quot;st&quot;/&gt;a) %*%&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; -&lt;span class=&quot;st&quot;/&gt;b)
mat &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(FF, TF, FT, TT), &lt;span class=&quot;dt&quot;&gt;nrow =&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
mat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]   29   32
## [2,]   17   22&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This contingency table (analogous to a confusion matrix) serves as the basis for the Fisher’s Exact Test as well as the &lt;span class=&quot;math inline&quot;&gt;\(\chi^2\)&lt;/span&gt; Test. Stated broadly, these test for a significant association between two (or more) presumedly independent sets.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;fisher.test&lt;/span&gt;(mat)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## 
##  Fisher's Exact Test for Count Data
## 
## data:  mat
## p-value = 0.8373
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.4850262 2.8537128
## sample estimates:
## odds ratio 
##   1.170924&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;logical-unions&quot; class=&quot;section level3&quot; readability=&quot;14&quot;&gt;&lt;h3&gt;Logical unions&lt;/h3&gt;&lt;p&gt;We can also use the dot product to tabulate the frequency of the logical unions between two vectors:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[|\mathbf{A} \cup \mathbf{B}| = \sum_{i=1}^na_i \lor b_i = \mathbf{a} \cdot \mathbf{a} + \mathbf{b} \cdot \mathbf{b} - \mathbf{a} \cdot \mathbf{b}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We can calculate this in R by computing on a boolean vector. We compare this to using the dot product.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;## [1] 71&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;a %*%&lt;span class=&quot;st&quot;/&gt;a +&lt;span class=&quot;st&quot;/&gt;b %*%&lt;span class=&quot;st&quot;/&gt;b -&lt;span class=&quot;st&quot;/&gt;a %*%&lt;span class=&quot;st&quot;/&gt;b&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,]   71&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;overlap-coefficient&quot; class=&quot;section level3&quot; readability=&quot;29&quot;&gt;&lt;h3&gt;Overlap coefficient&lt;/h3&gt;&lt;p&gt;The dot product also appears in the lesser known, but still important, overlap coefficient. This metric measures the degree to which two sets overlap. The overlap coefficient equation plays a role in network analysis where each “set” represents the connectivity of one element (called a &lt;em&gt;node&lt;/em&gt;) to all other elements. This provides a way to quantify the amount of common “links” between any two nodes in a network (Ravasz 2002).&lt;/p&gt;&lt;p&gt;The overlap coefficient is useful when analyzing biological networks because distinct biological features (e.g., genes) might have similar functional roles if they share a large number of overlapping partners (even if two do not interact directly). A high overlap coefficient means that the two nodes belong to the same “neighborhood”, regardless of whether the nodes are “neighbors” themselves (Yip 2007). For two binary vectors, the overlap coefficient equals the frequency of the logical intersections as “scaled” by the maximum possible number of intersections:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\textrm{overlap}(\mathbf{A},\mathbf{B}) = \frac{|\mathbf{A} \cap \mathbf{B}|}{\textrm{min}(|\mathbf{A}|,|\mathbf{B}|)}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Here, the notation &lt;span class=&quot;math inline&quot;&gt;\(|\mathbf{A}| = \sqrt{\mathbf{A} \cdot \mathbf{A}}\)&lt;/span&gt; (yet another &lt;em&gt;norm&lt;/em&gt;) denotes the sum of the absolute value of the elements in the vector.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;inner-product&quot; class=&quot;section level3&quot; readability=&quot;95.5&quot;&gt;&lt;h3&gt;Inner product&lt;/h3&gt;&lt;p&gt;Finally, I want to mention that although we discussed the dot product as it pertains to vectors, this operation applies to matrices as well. The dot product of two matrices, called the &lt;strong&gt;inner product&lt;/strong&gt;, is defined as the dot product of the i-th &lt;strong&gt;row&lt;/strong&gt; vector of the first matrix and the j-th &lt;strong&gt;column&lt;/strong&gt; vector of the second matrix (for each row of the first matrix and each column of the second matrix). This means that, unlike the vector dot product, the matrix inner product is not commutative: &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\cdot\mathbf{B}\neq\mathbf{B}\cdot\mathbf{A}\)&lt;/span&gt;. Its definition also requires the first matrix to have the same number of columns as the second has rows.&lt;/p&gt;&lt;p&gt;Given a matrix &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt; with &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; columns and a matrix &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{B}\)&lt;/span&gt; with &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; rows, the i,j-th result of the inner product equals the dot product of the i-th row of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt; and the j-th column of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{B}\)&lt;/span&gt;:&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[(\mathbf{A}\mathbf{B})_{i,j} = \mathbf{A}_{i,} \cdot \mathbf{B}_{, j}\]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;This might look less cryptic in R code. Note that we use the same &lt;code&gt;%*%&lt;/code&gt; operator for matrix multiplication.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;A &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;^&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;)
B &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;^&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;)
(A %*%&lt;span class=&quot;st&quot;/&gt;B)[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] -0.05027494&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;A[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, ] %*%&lt;span class=&quot;st&quot;/&gt;B[, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,] -0.05027494&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As an example, we will show how to use the inner product to repeat a calculation across the combination of every column vector. Specifically, we will calculate all covariances for a matrix. This is akin to what is achieved by the &lt;code&gt;cov&lt;/code&gt; function in R.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  3.0528358  2.3816630 -1.2354287  0.3243693  0.5110176
## [2,]  2.3816630  2.1267862 -0.5191054  0.2825757  0.2067718
## [3,] -1.2354287 -0.5191054  1.3062685 -0.1832324 -0.7521320
## [4,]  0.3243693  0.2825757 -0.1832324  0.2055409  0.2846195
## [5,]  0.5110176  0.2067718 -0.7521320  0.2846195  1.1981719&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To calculate covariance, we first need to “scale” each column by the column mean. We can do this using the &lt;code&gt;apply&lt;/code&gt; function. Next, since the inner product computes &lt;strong&gt;rows&lt;/strong&gt; by &lt;strong&gt;columns&lt;/strong&gt;, and we want to compute &lt;strong&gt;columns&lt;/strong&gt; by &lt;strong&gt;columns&lt;/strong&gt;, we need to transpose the first matrix (i.e., in order to have the rows contain column data). Then, we use the &lt;code&gt;%*%&lt;/code&gt; operator to compute the covariance matrix.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;9&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;As &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;apply&lt;/span&gt;(A, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, function(x) x -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(x))
&lt;span class=&quot;kw&quot;&gt;t&lt;/span&gt;(As) %*%&lt;span class=&quot;st&quot;/&gt;As /&lt;span class=&quot;st&quot;/&gt;(&lt;span class=&quot;kw&quot;&gt;nrow&lt;/span&gt;(As) -&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  3.0528358  2.3816630 -1.2354287  0.3243693  0.5110176
## [2,]  2.3816630  2.1267862 -0.5191054  0.2825757  0.2067718
## [3,] -1.2354287 -0.5191054  1.3062685 -0.1832324 -0.7521320
## [4,]  0.3243693  0.2825757 -0.1832324  0.2055409  0.2846195
## [5,]  0.5110176  0.2067718 -0.7521320  0.2846195  1.1981719&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that using the dot product in R is much faster than nested &lt;code&gt;for&lt;/code&gt; loops. In fact, the performance gain from the dot product can hold true even for low level languages like C++ if using a highly optimized linear algebra library (e.g., see &lt;code&gt;RcppEigen&lt;/code&gt;). In cases where a specific function like &lt;code&gt;cov&lt;/code&gt; is not available, dot products can make code run faster and look neater. Nevertheless, thinking in terms of the dot product offers a useful way to unify seemingly disparate statistical concepts.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;references&quot; class=&quot;section level3&quot;&gt;&lt;h3&gt;References&lt;/h3&gt;&lt;ol readability=&quot;4.0625&quot;&gt;&lt;li readability=&quot;6.6218487394958&quot;&gt;&lt;p&gt;Ravasz, E., A. L. Somera, D. A. Mongru, Z. N. Oltvai, and A.-L. Barabási. “Hierarchical Organization of Modularity in Metabolic Networks.” Science 297, no. 5586 (August 30, 2002): 1551–55. &lt;a href=&quot;http://dx.doi.org/10.1126/science.1073374&quot; class=&quot;uri&quot;&gt;http://dx.doi.org/10.1126/science.1073374&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li readability=&quot;1.5876288659794&quot;&gt;&lt;p&gt;Yip, Andy M., and Steve Horvath. “Gene Network Interconnectedness and the Generalized Topological Overlap Measure.” BMC Bioinformatics 8 (2007): 22. &lt;a href=&quot;http://dx.doi.org/10.1186/1471-2105-8-22&quot; class=&quot;uri&quot;&gt;http://dx.doi.org/10.1186/1471-2105-8-22&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blockads.fivefilters.org&quot;&gt;Let's block ads!&lt;/a&gt;&lt;/strong&gt; &lt;a href=&quot;https://blockads.fivefilters.org/acceptable.html&quot;&gt;(Why?)&lt;/a&gt;&lt;/p&gt;</description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.tpq.me/the-ubiquitous-dot-product.html</dc:identifier>
</item>
<item>
<title>R Parallelization through Batch Processing</title>
<link>http://www.tpq.me/r-parallelization-through-batch-processing.html</link>
<guid isPermaLink="true" >http://www.tpq.me/r-parallelization-through-batch-processing.html</guid>
<description>&lt;!-- tabsets --&gt;&lt;!-- code folding --&gt;&lt;!--/.navbar --&gt;&lt;div id=&quot;r-an-unparalleled-success&quot; class=&quot;section level3&quot; readability=&quot;36.893320518981&quot;&gt;&lt;h3&gt;R: an unparalleled success&lt;/h3&gt;&lt;p&gt;R as a data science tool has surpassed its alphabetical and reptilian competitors across many domains. However, its performance suffers tremendously from a lack of native parallel support. Although some packages (e.g., &lt;code&gt;foreach&lt;/code&gt; and &lt;code&gt;doParallel&lt;/code&gt;) have gone a long way in providing parallel computing for R, these plug-and-play parallelization packages have some key disadvantages. First, these packages often use special syntax that may require heavy revision of already drafted code. Second, without extreme attention to detail, these packages tend to invoke scope errors in the setting of layered dependencies. Third, these packages use frameworks that do not necessarily scale to large multi-core clusters.&lt;/p&gt;&lt;p&gt;In this article, we introduce another way to parallelize: through &lt;em&gt;batch processing&lt;/em&gt;. With batch processing, instead of executing a large task as one multi-threaded process, we break up the task into multiple single-threaded processes called &lt;em&gt;jobs&lt;/em&gt;. We then pass each job to a batch processing &lt;em&gt;job manager&lt;/em&gt; to hold in a queue until resources become available. This allows each single-threaded process to eventually get its own node for analysis. Although job managers are most often used in high-performance computing as a convenient way to share a single resource among multiple users, it is possible to set up a job manager like &lt;em&gt;TORQUE&lt;/em&gt; on a personal computer. Since this lacks an easy installation, I put together this run-at-your-own-risk &lt;a href=&quot;http://www.tpq.me/scripts/torque.bash&quot;&gt;script&lt;/a&gt; to install &lt;em&gt;TORQUE&lt;/em&gt; on Ubuntu 16.04.&lt;/p&gt;&lt;p&gt;Although a complete review of batch processing is beyond the scope of this article (and frankly beyond the scope of my expertise), this tutorial will hopefully show you how easy it is to harness &lt;em&gt;TORQUE&lt;/em&gt; to parallelize R scripts. However, keep in mind that the method discussed here relies on the R &lt;code&gt;system&lt;/code&gt; function to execute bash commands in the OS console. As such, this pipeline may not work outside of the Linux environment. Also, if you ever encounter a connection error, take note that you may need to reconfigure the &lt;code&gt;/etc/hosts&lt;/code&gt; file with an updated public IP address.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;from-cluster-to-cluster&quot; class=&quot;section level3&quot; readability=&quot;47.5&quot;&gt;&lt;h3&gt;From cluster to cluster&lt;/h3&gt;&lt;p&gt;Parallelization of R through batch processing involves two steps. First, we obviate the burden of having to write a parallel-process script by writing a script that writes single-process scripts. Second, we deliver each new script to the batch processing queue where it will wait to get executed on an unoccupied node.&lt;/p&gt;&lt;p&gt;In this endeavor, we make use of two helper functions, &lt;code&gt;writeR&lt;/code&gt; and &lt;code&gt;qsub&lt;/code&gt;, that simplify these two steps, respectively. I have made these available through the &lt;code&gt;miSciTools&lt;/code&gt; package, an R library that bundles miscellaneous software tools to expedite scientific analyses. We can install &lt;code&gt;miSciTools&lt;/code&gt; directly from GitHub using the &lt;code&gt;devtools&lt;/code&gt; package.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;devtools::&lt;span class=&quot;kw&quot;&gt;install_github&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&quot;tpq/miSciTools&quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(miSciTools)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, to show the logic behind &lt;em&gt;batch parallelization&lt;/em&gt;, we create a computationally expensive &lt;code&gt;for&lt;/code&gt;-loop that clusters 10 large mock datasets. In this example, parallelization is easy enough to implement using an R package like &lt;code&gt;foreach&lt;/code&gt;. However, for illustration purposes, we will instead parallelize this task through batch processing. Consider the single-threaded loop below.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;14&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;for(i in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;){
  
  N &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;2000&lt;/span&gt;
  someData &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N^&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;), N, N)
  hc &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;hclust&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;dist&lt;/span&gt;(someData))
  cuts &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;cutree&lt;/span&gt;(hc, &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)
  &lt;span class=&quot;kw&quot;&gt;write.csv&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;data.frame&lt;/span&gt;(cuts),
            &lt;span class=&quot;dt&quot;&gt;file =&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&quot;cluster-&quot;&lt;/span&gt;, i, &lt;span class=&quot;st&quot;&gt;&quot;.csv&quot;&lt;/span&gt;))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;During each pass through the loop, a large dataset is created, clustered, and labelled. In a more realistic scenario, each iteration might import and pre-process a different dataset. Since calculating the Euclidean distance of a dataset carries a high computational burden, we will almost certainly benefit here from parallelization.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;r-writing-r&quot; class=&quot;section level3&quot; readability=&quot;46.867924528302&quot;&gt;&lt;h3&gt;R writing R&lt;/h3&gt;&lt;p&gt;In this example, we break up the task into 10 parallel parts by writing a single script that writes 10 separate scripts. Each script will generate a random matrix, cluster the data into 10 groups, and then save the cluster labels. We simplify this task by using &lt;code&gt;writeR&lt;/code&gt; which creates an R script from any number of “free text” representations of R code with intermingled variables from the &lt;em&gt;parent environment&lt;/em&gt; (i.e., the environment where the new script is written). To generate a preview of how the R script will appear when saved, we toggle the argument &lt;code&gt;preview = TRUE&lt;/code&gt;. The excerpt below will hopefully clarify the behavior of this function in a way that practical English cannot. Keep in mind that extraneous space does not impact R code in any way.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;15&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;outsideCode =&lt;span class=&quot;st&quot;&gt; &quot;something else from outside&quot;&lt;/span&gt;
file &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;writeR&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;          this = &quot;the first line of code&quot;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;          and = &quot;then, there is a second line of code&quot;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;          that = paste(&quot;you can also add&quot;, &quot;'&lt;/span&gt;, outsideCode, &lt;span class=&quot;st&quot;&gt;'&quot;,&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;                       &quot;as long as you remember punctuation&quot;)'&lt;/span&gt;
  , &lt;span class=&quot;dt&quot;&gt;preview =&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We see in this preview an additional line of code that we did not provide explicitly. The &lt;code&gt;load()&lt;/code&gt; command is added to each new script so that it automatically loads the &lt;em&gt;parent environment&lt;/em&gt;. This ensures that any variables (or functions or packages) not explicitly passed to the new script will remain available to procedures within the &lt;code&gt;for&lt;/code&gt;-loop. Note that the new script saves the &lt;em&gt;parent environment&lt;/em&gt; at the time in which the script is written, which you can exploit in order to create a unique &lt;em&gt;child environment&lt;/em&gt; for each script.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;writeR&lt;/code&gt; function also accepts the optional argument, &lt;code&gt;file&lt;/code&gt;, which allows the user to change the location and name of the temporary R script. By default, all new R scripts (along with the .RData file from the &lt;em&gt;parent environment&lt;/em&gt;) are saved in a temporary directory. Note that this directory, along with its contents, gets deleted upon termination of the parent R session.&lt;/p&gt;&lt;p&gt;Finally, keep in mind that &lt;code&gt;writeR&lt;/code&gt; generates a new file using &lt;a href=&quot;https://en.wikipedia.org/wiki/Escape_character&quot;&gt;escaped&lt;/a&gt; text (e.g., try running &lt;code&gt;cat(&quot;\&quot;Quotes\&quot; and Tabs?\n\tYes.&quot;)&lt;/code&gt; in the R console). Since you need to wrap the “free text” R code within a set of quotes, you may need to use escaped quotations if you use more than one quotation style within your code (i.e., double quotes &lt;em&gt;and&lt;/em&gt; single quotes). To avoid having to escape quotations, stick to one style when writing R code for batch processing. Otherwise, new lines and blank space included in the “free text” R code will get written to the new file automatically.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;r-bashing-r&quot; class=&quot;section level3&quot; readability=&quot;32&quot;&gt;&lt;h3&gt;R bashing R&lt;/h3&gt;&lt;p&gt;To use &lt;code&gt;qsub&lt;/code&gt;, we can provide either a “free text” bash command or the file location of an R script. The function will then deliver this command (or script) to the batch processing &lt;em&gt;queue&lt;/em&gt;. In the latter case, this function pipes an &lt;code&gt;R CMD BATCH&lt;/code&gt; bash command to the &lt;code&gt;qsub&lt;/code&gt; bash command.&lt;/p&gt;&lt;p&gt;To test that &lt;code&gt;qsub&lt;/code&gt; works properly on your machine, try the following function call. If successful, a new process should appear in the queue. You view the queue from the OS console using the &lt;code&gt;qstat&lt;/code&gt; bash command.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;qsub&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&quot;sleep 30&quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, we can use &lt;code&gt;qsub&lt;/code&gt; to put &lt;code&gt;writeR&lt;/code&gt; to work. Note that we supply &lt;code&gt;i&lt;/code&gt; from outside of the &lt;code&gt;writeR&lt;/code&gt; environment. However, because each new &lt;code&gt;writeR&lt;/code&gt; script imports the working directory of the parent script, we could just as well have named the output files using &lt;code&gt;file = paste0(&quot;cluster-&quot;, i, &quot;.csv&quot;)&lt;/code&gt;.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;17&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;for(i in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;){
  
  cmd &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;writeR&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;  N &amp;lt;- 2000&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;  someData &amp;lt;- matrix(rnorm(N^2), N, N)&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;  hc &amp;lt;- hclust(dist(someData))&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;  cuts &amp;lt;- cutree(hc, 10)&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;  write.csv(data.frame(cuts),&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;            file = paste0(&quot;cluster-&quot;, '&lt;/span&gt;, i, &lt;span class=&quot;st&quot;&gt;', &quot;.csv&quot;))&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;  '&lt;/span&gt;)
  
  &lt;span class=&quot;kw&quot;&gt;qsub&lt;/span&gt;(cmd)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&quot;a-place-in-the-queue&quot; class=&quot;section level3&quot; readability=&quot;24.5&quot;&gt;&lt;h3&gt;A place in the queue&lt;/h3&gt;&lt;p&gt;When this &lt;code&gt;for&lt;/code&gt;-loop completes, 10 single-process R scripts will have joined the queue. From the console, you can check the &lt;em&gt;TORQUE&lt;/em&gt; queue using the &lt;code&gt;qstat&lt;/code&gt; bash command. In addition, you can use the &lt;code&gt;qdel&lt;/code&gt; bash command to remove a queued job and the &lt;code&gt;qrun&lt;/code&gt; bash command to force the execution of a queued job.&lt;/p&gt;&lt;p&gt;In this example, each script saves the cluster labels as a comma-delimited file in the working directory. By default, the working directory of batch processed R scripts is the home directory of the computer. To combine the results from the parallelized processes, we could write a simple loop that reads in the &lt;code&gt;.csv&lt;/code&gt; file and joins the contents.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;files &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;vector&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&quot;list&quot;&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)
for(i in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;){
  
  files[[i]] &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;read.csv&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&quot;cluster-&quot;&lt;/span&gt;, i, &lt;span class=&quot;st&quot;&gt;&quot;.csv&quot;&lt;/span&gt;))
}

&lt;span class=&quot;kw&quot;&gt;do.call&lt;/span&gt;(rbind, files)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&quot;scaling-to-an-hpc&quot; class=&quot;section level3&quot; readability=&quot;24&quot;&gt;&lt;h3&gt;Scaling to an HPC&lt;/h3&gt;&lt;p&gt;When using a job manager on a high-performance computer (HPC), system administrators often request that users provide additional parameters that help guide optimal resource utilization. This includes, for example, the anticipated run time or expected RAM overhead. The &lt;code&gt;qsub&lt;/code&gt; function for R will pass along any number of specified &lt;em&gt;TORQUE&lt;/em&gt; parameters to the OS console: simply provide them as additional arguments. For example, to replicate the &lt;em&gt;TORQUE&lt;/em&gt; command &lt;code&gt;qsub -M thom@tpq.me [someBashCmd]&lt;/code&gt;, call instead the R function &lt;code&gt;qsub(someBashCmd, M = &quot;thom@tpq.me&quot;)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Depending on how you access the HPC, you may not have an R IDE like RStudio. Instead, you may need to use the OS console to run the master script-that-writes-scripts. To do this, simply call &lt;code&gt;R CMD BATCH script-that-writes-scripts.R&lt;/code&gt; from the OS console. This will execute the R script and pass the individual processes to the job manager.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blockads.fivefilters.org&quot;&gt;Let's block ads!&lt;/a&gt;&lt;/strong&gt; &lt;a href=&quot;https://blockads.fivefilters.org/acceptable.html&quot;&gt;(Why?)&lt;/a&gt;&lt;/p&gt;</description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.tpq.me/r-parallelization-through-batch-processing.html</dc:identifier>
</item>
<item>
<title>Reeling in Fisher&amp;#039;s Z Transformation</title>
<link>http://www.tpq.me/reeling-in-fishers-z-transformation.html</link>
<guid isPermaLink="true" >http://www.tpq.me/reeling-in-fishers-z-transformation.html</guid>
<description>&lt;!-- tabsets --&gt;&lt;!-- code folding --&gt;&lt;!--/.navbar --&gt;&lt;div id=&quot;a-fisher-to-fry&quot; class=&quot;section level3&quot; readability=&quot;30.465517241379&quot;&gt;&lt;h3&gt;A Fisher to fry&lt;/h3&gt;&lt;p&gt;I write now as a premise to another project rooted in the construction of an empiric distribution for a novel measure of dependence. For my own sake, I set out here to evaluate Fisher’s hypothesis that the &lt;em&gt;inverse hyperbolic tangent&lt;/em&gt; of a sample correlation coefficient, &lt;span class=&quot;math inline&quot;&gt;\(r\)&lt;/span&gt;, follows a normal distribution with a population mean equal to the similarly transformed population correlation coefficient, and a sampling error equal to &lt;span class=&quot;math inline&quot;&gt;\(\frac{1}{\sqrt{N-3}}\)&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;This transformation, known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;Fisher z-transformation&lt;/a&gt;, makes it possible to use correlation coefficients within a conventional hypothesis testing framework by providing a way to calculate standard error using only a point estimate (i.e., the sample correlation coefficient) and the sample size. In this way, it offers an elegant means by which to effortlessly apply well-described statistical methods to correlation-based studies.&lt;/p&gt;&lt;p&gt;In this report, I demonstrate empirically that (1) the z-transformation of a distribution of sample correlation coefficients has a normal distribution, and (2) the standard error about that distribution approximately equals &lt;span class=&quot;math inline&quot;&gt;\(\frac{1}{\sqrt{N-3}}\)&lt;/span&gt;. Although not discussed further, do take note that Fisher’s hypothesis presupposes that the two correlated variables are independent and follow a bivariate normal distribution.&lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;it-begins-with-the-first-step&quot; class=&quot;section level3&quot; readability=&quot;39&quot;&gt;&lt;h3&gt;It begins with the first step&lt;/h3&gt;&lt;p&gt;We begin by calculating the correlation between two randomly distributed &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;-sized vectors, &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;8&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;N &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;
X &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N)
Y &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N)
&lt;span class=&quot;kw&quot;&gt;cor&lt;/span&gt;(X, Y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 0.0106989&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Yet, a single correlation coefficient is not sufficient to put Fisher’s z-transformation to the test. Instead, we must establish a distribution of correlation coefficients. For this, we replicate the above code chunk an arbitrarily large number of times to build a sample set of correlation coefficients between 5000 pairs of &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;-sized vectors.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;10&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;N &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;
res &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(N, &lt;span class=&quot;dv&quot;&gt;5000&lt;/span&gt;), function(N) &lt;span class=&quot;kw&quot;&gt;cor&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N), &lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To apply Fisher’s z-transformation, we simply calculate &lt;span class=&quot;math inline&quot;&gt;\(atanh(r)\)&lt;/span&gt; for this sample set of correlation coefficients, yielding a normal distribution of sample statistics. The Shapiro-Wilk normality test confirms that these z-transformed data follow a normal distribution.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;density&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(res)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&quot;http://www.tpq.me/reeling-in-fishers-z-transformation_files/figure-html/unnamed-chunk-3-1.png&quot; width=&quot;672&quot;/&gt;&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;shapiro.test&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(res))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  atanh(res)
## W = 0.99969, p-value = 0.6889&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Finally, since the &lt;em&gt;standard error&lt;/em&gt; of a statistic is defined as the &lt;em&gt;standard deviation of the sampling distribution&lt;/em&gt; of that statistic, we would expect that the standard deviation of these z-transformed data approximately equals &lt;span class=&quot;math inline&quot;&gt;\(\frac{1}{\sqrt{N-3}}\)&lt;/span&gt;. In other words, the empiric standard error should approximate the predicted standard error.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sd&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(res))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 0.03163743&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;/(&lt;span class=&quot;kw&quot;&gt;sqrt&lt;/span&gt;(N&lt;span class=&quot;dv&quot;&gt;-3&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 0.03167032&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;one-size-to-fit-them-all&quot; class=&quot;section level3&quot; readability=&quot;27&quot;&gt;&lt;h3&gt;One size to fit them all&lt;/h3&gt;&lt;p&gt;Everything checks out so far, but does Fisher’s z-transformation rule hold true for all &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;-sized vectors? To test this, we embed the code that calculates correlation between 5000 pairs of &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;-sized vectors within a loop that iterates across several values of &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;14&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;N.all &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;seq&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;from =&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;to =&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;by =&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
distrs &amp;lt;-
&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;lapply&lt;/span&gt;(N.all,
         function(N){
           
           &lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(N, &lt;span class=&quot;dv&quot;&gt;5000&lt;/span&gt;), function(N) &lt;span class=&quot;kw&quot;&gt;cor&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N), &lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N)))
         })&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using this result, we can calculate both the empiric standard error and the predicted standard error for all &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;10&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;sd.empiric &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(distrs, function(d) &lt;span class=&quot;kw&quot;&gt;sd&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(d)))
sd.predict &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(N.all, function(N) &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;/(&lt;span class=&quot;kw&quot;&gt;sqrt&lt;/span&gt;(N&lt;span class=&quot;dv&quot;&gt;-3&lt;/span&gt;)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, we compare these quantitatively by calculating the mean of the percent difference between them.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;abs&lt;/span&gt;(sd.empiric -&lt;span class=&quot;st&quot;/&gt;sd.predict)/sd.predict*&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 0.9554196&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div id=&quot;positively-certain&quot; class=&quot;section level3&quot; readability=&quot;49.5&quot;&gt;&lt;h3&gt;Positively certain&lt;/h3&gt;&lt;p&gt;Now, all that is left to do is convince myself that the approximate equality between the empiric standard error and the predicted standard error is not just an artifact of the distribution of the random variables &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;. To do this, we apply the pipeline established above to two positively correlated random variables. First, however, we need to define a function that creates the two positively correlated random variables, &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;, containing &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; elements each, and then calculates the correlation between them.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;newCor &amp;lt;-&lt;span class=&quot;st&quot;/&gt;function(N){
  
  v &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N, &lt;span class=&quot;dt&quot;&gt;mean =&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)
  X &amp;lt;-&lt;span class=&quot;st&quot;/&gt;v +&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N, &lt;span class=&quot;dt&quot;&gt;sd =&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;0.25&lt;/span&gt;)
  Y &amp;lt;-&lt;span class=&quot;st&quot;/&gt;v +&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(N, &lt;span class=&quot;dt&quot;&gt;sd =&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;0.25&lt;/span&gt;)
  &lt;span class=&quot;kw&quot;&gt;cor&lt;/span&gt;(X, Y)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using this formula, we now have a tidy way of generating a distribution of positive correlation coefficients.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;9&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;N &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;
res &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(N, &lt;span class=&quot;dv&quot;&gt;5000&lt;/span&gt;), newCor)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, the Shapiro-Wilk normality test confirms that the z-transformed data follow a normal distribution.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;density&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(res)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&quot;http://www.tpq.me/reeling-in-fishers-z-transformation_files/figure-html/unnamed-chunk-10-1.png&quot; width=&quot;672&quot;/&gt;&lt;/p&gt;&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;shapiro.test&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(res))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  atanh(res)
## W = 0.99961, p-value = 0.4322&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we generate a distribution of positive correlations between two &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;-sized random variables across a range of &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;N.all &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;seq&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
distrs &amp;lt;-
&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;lapply&lt;/span&gt;(N.all,
         function(N){
           
           &lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(N, &lt;span class=&quot;dv&quot;&gt;5000&lt;/span&gt;), newCor)
         })&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As above, we compare the empiric standard error with the predicted standard error…&lt;/p&gt;&lt;div class=&quot;sourceCode&quot; readability=&quot;10&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;sd.empiric &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(distrs, function(d) &lt;span class=&quot;kw&quot;&gt;sd&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;atanh&lt;/span&gt;(d)))
sd.predict &amp;lt;-&lt;span class=&quot;st&quot;/&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(N.all, function(N) &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;/(&lt;span class=&quot;kw&quot;&gt;sqrt&lt;/span&gt;(N&lt;span class=&quot;dv&quot;&gt;-3&lt;/span&gt;)))
&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;abs&lt;/span&gt;(sd.empiric -&lt;span class=&quot;st&quot;/&gt;sd.predict)/sd.predict*&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## [1] 0.8095219&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;…and breathe a sigh of relief.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blockads.fivefilters.org&quot;&gt;Let's block ads!&lt;/a&gt;&lt;/strong&gt; &lt;a href=&quot;https://blockads.fivefilters.org/acceptable.html&quot;&gt;(Why?)&lt;/a&gt;&lt;/p&gt;</description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.tpq.me/reeling-in-fishers-z-transformation.html</dc:identifier>
</item>
</channel>
</rss>