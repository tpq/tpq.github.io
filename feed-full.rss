<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<atom:link rel="self" href="http://ftr.fivefilters.org/makefulltextfeed.php?url=tpq.github.io%2Ffeed.rss&amp;links=preserve" />
<atom:link rel="alternate" title="Source URL" href="http://tpq.github.io/feed.rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2Fftr.fivefilters.org%2Fmakefulltextfeed.php%3Furl%3Dtpq.github.io%252Ffeed.rss%26links%3Dpreserve&amp;back=http%3A%2F%2Fftr.fivefilters.org%2Fmakefulltextfeed.php%3Furl%3Dtpq.github.io%252Ffeed.rss%26links%3Dpreserve" />
<title>tpq.me</title>
<link>http://www.tpq.me</link>
<description>Personal website of Thom Quinn</description>
<item>
<title>Algebra Is for Those Without Algorithms</title>
<link>http://www.tpq.me/algebra-is-for-those-without-algorithms.html</link>
<guid isPermaLink="true" >http://www.tpq.me/algebra-is-for-those-without-algorithms.html</guid>
<description>&lt;!-- tabsets --&gt;


&lt;!-- code folding --&gt;






&lt;!--/.navbar --&gt;




&lt;div id=&quot;astable-mode&quot; class=&quot;section level3&quot; readability=&quot;34&quot;&gt;
&lt;h3&gt;Astable mode&lt;/h3&gt;
&lt;p&gt;On a whim, I traded a handful of tokens imbued with symbolic value (and acquired arbitrarily in exchange for my labor) for a packet of shaped metal. Inside this pack were veterans of the Space Age: resistors, capacitors, transistors, and an integrated circuit. The last of these pieces, merely the size of a fingernail, Man had manufactured to keep time (just in case, I suppose, He might otherwise find it slip away?). I have no regard for clocks, but timer circuits can also make sound: as the regular oscillations of current pass through a speaker, they vibrate the air such that we, by a trick of anatomy and signal processing, can hear noise and assign it an emotion.&lt;/p&gt;
&lt;p&gt;I want to make sound. This is why I purchased a 555 timer integrated circuit. These circuits have 8-pins that, when wired in a certain configuration, can make sound. A 555 has two modes. The first, astable mode, outputs a continuous oscillating current when a switch is pressed. The second, monostable mode, outputs a continuous non-oscillating current for a fixed period of time when a switch is pressed. We will use the 555 timer in astable mode to build a circuit that synthesizes sound (i.e., a synthesizer).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;sales-pitch&quot; class=&quot;section level3&quot; readability=&quot;35&quot;&gt;
&lt;h3&gt;Sales pitch&lt;/h3&gt;
&lt;p&gt;Pitch is determined by the oscillatory frequency of the output current. We can calculate this frequency based on three parts labelled &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. Measuring resistance in &lt;span class=&quot;math inline&quot;&gt;\(\textrm{k} \Omega\)&lt;/span&gt; and capacitance in &lt;span class=&quot;math inline&quot;&gt;\(\mu \textrm{F}\)&lt;/span&gt;, the formula is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[ f = \frac{1440}{(R_1 + 2R_2)C} \]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;9&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;f &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(r1, r2, &lt;span class=&quot;dt&quot;&gt;c =&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;.01&lt;/span&gt;){
  
  &lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;1.44&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;((r1 &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;r2) &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;c)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With &lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt; fixed, it is easy enough to find &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt; for a single frequency. However, to make a multi-tone synthesizer, we will need to alter either &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt; for each note. In practice, we can achieve this by having each piano key add a little bit of resistance to &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt; (i.e., via a series configuration). For example, if &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt; outputs middle C, then &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(R_2 + R_A\)&lt;/span&gt; might output B a semi-tone lower.&lt;/p&gt;
&lt;p&gt;The question becomes: how do we find the values of &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(R_A\)&lt;/span&gt; such that the addition of each new &lt;span class=&quot;math inline&quot;&gt;\(R_A\)&lt;/span&gt; (i.e., &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; added resistors) makes &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(R_2 + kR_A\)&lt;/span&gt; equal the next note in the dichromatic scale?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;circuit-optimization&quot; class=&quot;section level3&quot; readability=&quot;59.902834008097&quot;&gt;
&lt;h3&gt;Circuit optimization&lt;/h3&gt;
&lt;p&gt;This looks like a problem that we can solve with optimization by way of a genetic algorithm. To simplify the issue, however, we will fix &lt;span class=&quot;math inline&quot;&gt;\(R_A\)&lt;/span&gt; to &lt;span class=&quot;math inline&quot;&gt;\(10 \textrm{k} \Omega\)&lt;/span&gt; (convenient, since I already own a bunch of these). First, however, we need to define a &amp;ldquo;ribosome&amp;rdquo; function that converts binary input into a list of integers (see &lt;a href=&quot;http://tpq.me/genetic-monkeys-on-typewriters.html&quot;&gt;here&lt;/a&gt; for what that means).&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;18&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ribosome &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(string, nbits,
                     &lt;span class=&quot;dt&quot;&gt;how =&lt;/span&gt; &lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(codon) &lt;span class=&quot;kw&quot;&gt;strtoi&lt;/span&gt;(codon, &lt;span class=&quot;dt&quot;&gt;base =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)){
  
  &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt;(&lt;span class=&quot;op&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;is.character&lt;/span&gt;(string)){
    string &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(string, &lt;span class=&quot;dt&quot;&gt;collapse =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;)
  }
  
  &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(string) &lt;span class=&quot;op&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;){
    &lt;span class=&quot;kw&quot;&gt;stop&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Digital gene not evenly divisible by chosen codon size.&amp;quot;&lt;/span&gt;)
  }
  
  output &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;vector&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;list&amp;quot;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(string)&lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;nbits)
  &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt;(i &lt;span class=&quot;cf&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(string)&lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)){
    
    codon &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;substr&lt;/span&gt;(string, i&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, i&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;nbits)
    output[[i&lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;do.call&lt;/span&gt;(how, &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(codon))
  }
  
  &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt;(output)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can define the &amp;ldquo;fitness&amp;rdquo; function. This will measure the error of each attempted solution. Then, the genetic algorithm will allow the better performing solutions to &amp;ldquo;reproduce&amp;rdquo; (with &amp;ldquo;crossing over&amp;rdquo;), potentially giving rise to even better solutions. Specifically, our fitness function measures how closely the predicted frequencies match the expected (i.e., desired) frequencies. Here, the expected frequencies correspond to the octave ascending from middle C. Note that we negate the error because genetic algorithms seek to maximize the result of the &amp;ldquo;fitness&amp;rdquo; function.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;26&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;fitness &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(binary){
  
  AA &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(binary, &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;16&lt;/span&gt;)
  
  predicted &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;13&lt;/span&gt;, &lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(i){
    &lt;span class=&quot;kw&quot;&gt;f&lt;/span&gt;(AA[[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]], AA[[&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]] &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;i&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;c =&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;)
  })
  
  expected &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;523.25&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;493.88&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;466.16&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;440.00&lt;/span&gt;, &lt;span class=&quot;co&quot;&gt;# semi-tones&lt;/span&gt;
                &lt;span class=&quot;fl&quot;&gt;415.30&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;392.00&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;369.99&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;349.23&lt;/span&gt;,
                &lt;span class=&quot;fl&quot;&gt;329.63&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;311.13&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;293.66&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;277.18&lt;/span&gt;,
                &lt;span class=&quot;fl&quot;&gt;261.63&lt;/span&gt;)
  
  error &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;((predicted &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;expected)&lt;span class=&quot;op&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
  &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt;(&lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;error)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can now call the genetic algorithm, then plot its performance over time. Here, I&amp;rsquo;ll run the algorithm for 1000 breeding cycles (although originally I used 50000).&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;11&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(GA)
&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)
res &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ga&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fitness =&lt;/span&gt; fitness, &lt;span class=&quot;dt&quot;&gt;nBits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;32&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;maxiter =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(res)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://tpq.github.io/algebra-is-for-those-without-algorithms_files/figure-html/unnamed-chunk-4-1.png&quot; width=&quot;672&quot;&gt;&lt;/p&gt;
&lt;p&gt;Often, genetic algorithms converge on multiple equally good solutions. Let&amp;rsquo;s view them all.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;13&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt;(i &lt;span class=&quot;cf&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nrow&lt;/span&gt;(res&lt;span class=&quot;op&quot;&gt;@&lt;/span&gt;solution)){
  &lt;span class=&quot;kw&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;r1&amp;quot;&lt;/span&gt; =&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(res&lt;span class=&quot;op&quot;&gt;@&lt;/span&gt;solution[i,], &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;16&lt;/span&gt;)[[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]],
          &lt;span class=&quot;st&quot;&gt;&amp;quot;r2&amp;quot;&lt;/span&gt; =&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(res&lt;span class=&quot;op&quot;&gt;@&lt;/span&gt;solution[i,], &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;16&lt;/span&gt;)[[&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]]))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##  r1  r2 
## 217  18 
##  r1  r2 
## 153  50&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;results&quot; class=&quot;section level3&quot; readability=&quot;16&quot;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;Using the second set of &lt;span class=&quot;math inline&quot;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(R_2\)&lt;/span&gt; values from above, the synthesizer works! Although fixing &lt;span class=&quot;math inline&quot;&gt;\(R_A\)&lt;/span&gt; seems to prevent the model from converging on a perfect solution, the synthesizer only wobbles out of tune by a few cents in each direction. When playing melodies, I do not notice the sharpening or flattening of any notes. Although using a fixed &lt;span class=&quot;math inline&quot;&gt;\(R_A\)&lt;/span&gt; made it much easier to assemble the circuit, a post-hoc evaluation of the predicted frequencies suggests that a fixed &lt;span class=&quot;math inline&quot;&gt;\(R_A\)&lt;/span&gt; might not work for a synthesizer that spans multiple octaves.&lt;/p&gt;
&lt;div class=&quot;figure&quot; readability=&quot;7&quot;&gt;
&lt;img src=&quot;https://tpq.github.io/astable-me.jpg&quot; alt=&quot;Figure 1. Me holding a DIY synthesizer tuned to C major.&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 1. Me holding a DIY synthesizer tuned to C major.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot; readability=&quot;7&quot;&gt;
&lt;img src=&quot;https://tpq.github.io/astable.jpg&quot; alt=&quot;Figure 2. The 555 timer circuit in astable mode.&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Figure 2. The 555 timer circuit in astable mode.&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;





&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blockads.fivefilters.org&quot;&gt;Let's block ads!&lt;/a&gt;&lt;/strong&gt; &lt;a href=&quot;https://blockads.fivefilters.org/acceptable.html&quot;&gt;(Why?)&lt;/a&gt;&lt;/p&gt;</description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tpq.github.io/algebra-is-for-those-without-algorithms.html</dc:identifier>
</item>
<item>
<title>The Ubiquitous Dot Product</title>
<link>http://www.tpq.me/the-ubiquitous-dot-product.html</link>
<guid isPermaLink="true" >http://www.tpq.me/the-ubiquitous-dot-product.html</guid>
<description>&lt;!-- tabsets --&gt;


&lt;!-- code folding --&gt;






&lt;!--/.navbar --&gt;




&lt;div id=&quot;the-dot-in-common&quot; class=&quot;section level3&quot; readability=&quot;14&quot;&gt;
&lt;h3&gt;The dot in-common&lt;/h3&gt;
&lt;p&gt;Through my years of training in biomedicine, and despite taking a number of mathematics and statistics courses, I have somehow missed out on formal education in linear algebra. I feel that this has put me at a disadvantage in the age of next-generation sequencing where linear algebra plays an important role in analyzing large multi-dimensional vectors of genetic data. As part of catching up on a missed opportunity, I wanted to share what I have learned about the dot product and its ubiquity in statistical computing. In doing so, I hope the reader and writer can gain a better intuition of this fascinating operation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;sum-definitions&quot; class=&quot;section level3&quot; readability=&quot;36.80023995201&quot;&gt;
&lt;h3&gt;Sum definitions&lt;/h3&gt;
&lt;p&gt;Wikipedia offers a number of definitions for the dot product. Let us look at the algebraic definition:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^na_ib_i = a_1b_1+a_2b_2+\dots+a_nb_n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dot product operates on two vectors to sum the element-wise product of those vectors. This appears as a simple call in the natively vectorized R programming language.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;a &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
b &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(a &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [1] -6.367611&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could use the base R operator for the dot product.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] -6.367611&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below, we will see how the dot product appears in many of the statistical methods we use routinely. But what does the dot product signify? Gaining an intuition for this operation is considerably difficult. Several introductory websites provide a number of geometric explanations. For example, the &lt;a href=&quot;https://brilliant.org/wiki/dot-product-distance-between-point-and-a-line/&quot;&gt;Brilliant&lt;/a&gt; website provides a helpful interactive graph to illustrate how changing the direction or magnitude of two-dimensional vectors can change the dot product of those vectors.&lt;/p&gt;
&lt;p&gt;Otherwise, I find it helpful to think of the dot product as a weighted measure of the &amp;ldquo;agreement&amp;rdquo; between the two vectors in their joint departure from zero. Consider the case where the absolute value of every element in the vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; is less than or equal to the corresponding element in &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;: if &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; approaches &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;, then &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a} \cdot \mathbf{b}\)&lt;/span&gt; will approach the sum of the squares of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;. On the other hand, if &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; approaches the opposite of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;, then &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a} \cdot \mathbf{b}\)&lt;/span&gt; will approach the negative sum of the squares of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;euclidean-distance&quot; class=&quot;section level3&quot; readability=&quot;31.5&quot;&gt;
&lt;h3&gt;Euclidean distance&lt;/h3&gt;
&lt;p&gt;We will first take a look at Euclidean distance between two vectors, calculated as the square root of the element-wise differences squared. Let us look at the algebraic definition:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[d(\mathbf{a}, \mathbf{b}) = \sqrt{\sum_{i=1}^n(a_i - b_i)^2} = \sqrt{\sum_{i=1}^n(a_i^2 + b_i^2 - 2a_ib_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the second formulation, we see that if we break up the summation into three separate parts (i.e., &lt;span class=&quot;math inline&quot;&gt;\(\sum_ia_i^2\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\sum_ib_i^2\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(\sum_i2a_ib_i\)&lt;/span&gt;), we can rewrite Euclidean distance quite neatly using the dot product:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[d(\mathbf{a}, \mathbf{b}) = \sqrt{\mathbf{a} \cdot \mathbf{a} + \mathbf{b} \cdot \mathbf{b} - 2\mathbf{a}\cdot\mathbf{b}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can calculate Euclidean distance between two vectors in R using the &lt;code&gt;dist&lt;/code&gt; function. We compare this to the distance computed using the dot product.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;8&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;dist&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;t&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;data.frame&lt;/span&gt;(a, b)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##          a
## b 14.97394&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sqrt&lt;/span&gt;(a &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]
## [1,] 14.97394&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;variance&quot; class=&quot;section level3&quot; readability=&quot;44&quot;&gt;
&lt;h3&gt;Variance&lt;/h3&gt;
&lt;p&gt;Variance, or standard deviation squared, is fundamental to statistics, making up the foundation of hypothesis testing. Let us look at the algebraic definition:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\sigma^2 = \frac{\sum_i(a_i-\mu)^2}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Variance requires knowledge about two constants: the mean (&lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;) and vector length (&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;). The mean used here is the sum of the elements divided by the total number of elements. When calculating variance, we substract this constant from each element in the vector. In a way, we can think of this as &amp;ldquo;scaling&amp;rdquo; the original vector (i.e., &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}_S = \mathbf{a} - \mu_\mathbf{a}\)&lt;/span&gt;) so that the new mean equals zero.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;round&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, we can describe variance using the dot product:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\sigma^2 = \frac{\mathbf{a}_S \cdot \mathbf{a}_S}{N} = \frac{(\mathbf{a} - \mu_\mathbf{a}) \cdot (\mathbf{a} - \mu_\mathbf{a})}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We often calculate the dot product of a vector with itself. We can use the notation &lt;span class=&quot;math inline&quot;&gt;\(|\mathbf{a}|^2 = \mathbf{a} \cdot \mathbf{a}\)&lt;/span&gt; (called a &lt;em&gt;norm&lt;/em&gt;) to simplify this routine procedure. This allows us to define variance succintly:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\sigma^2 = \frac{|\mathbf{a} - \mu_\mathbf{a}|^2}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can calculate variance in R using the &lt;code&gt;var&lt;/code&gt; function. We compare this to the variance computed using the dot product. Note that, by default, the &lt;code&gt;var&lt;/code&gt; function in R computes &lt;em&gt;sample variance&lt;/em&gt;. This means it uses &lt;span class=&quot;math inline&quot;&gt;\(N - 1\)&lt;/span&gt; instead of &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; as the denominator.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## [1] 0.9248466&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;(a &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)) &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(a &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)) &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(a) &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] 0.9248466&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;covariance&quot; class=&quot;section level3&quot; readability=&quot;31&quot;&gt;
&lt;h3&gt;Covariance&lt;/h3&gt;
&lt;p&gt;Covariance is an extension of variance involving two vectors. Let us look at the algebraic definition:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\textrm{cov}(\mathbf{a}, \mathbf{b})= \frac{\sum_i(a_i-\mu_\mathbf{a})(b_i - \mu_\mathbf{b})}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Conceiving of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a} - \mu_\mathbf{a}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b} - \mu_\mathbf{b}\)&lt;/span&gt; as &amp;ldquo;scaled&amp;rdquo; vectors, we can rewrite covariance as a dot product:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\textrm{cov}(\mathbf{a}, \mathbf{b})= \frac{(\mathbf{a} - \mu_\mathbf{a}) \cdot (\mathbf{b} - \mu_\mathbf{b})}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can calculate covariance in R using the &lt;code&gt;cov&lt;/code&gt; function. We compare this to the covariance computed using the dot product. Note that, like &lt;code&gt;var&lt;/code&gt;, the &lt;code&gt;cov&lt;/code&gt; function in R computes &lt;em&gt;sample covariance&lt;/em&gt; with &lt;span class=&quot;math inline&quot;&gt;\(N-1\)&lt;/span&gt; as the denominator.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## [1] -0.05653278&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;(a &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(a)) &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(b &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(b)) &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(a) &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,] -0.05653278&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;correlation&quot; class=&quot;section level3&quot; readability=&quot;15&quot;&gt;
&lt;h3&gt;Correlation&lt;/h3&gt;
&lt;p&gt;Pearson&amp;rsquo;s correlation coefficient is really just a modification to covariance that &amp;ldquo;scales&amp;rdquo; it by the product of the individual variances. As such, we can substitute the equation below to define Pearson&amp;rsquo;s correlation coefficient in terms of the dot product:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\rho(\mathbf{a}, \mathbf{b}) = \frac{\textrm{cov}(\mathbf{a}, \mathbf{b})}{\sigma_\mathbf{a}\sigma_\mathbf{b}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;logical-intersections&quot; class=&quot;section level3&quot; readability=&quot;50&quot;&gt;
&lt;h3&gt;Logical intersections&lt;/h3&gt;
&lt;p&gt;Interestingly, if we use &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{b}\)&lt;/span&gt; to denote binary vectors, then we can use the dot product to count the frequency of the logical intersections between the vectors. For example, given a set with as many as &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; elements (where the vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{a}\)&lt;/span&gt; indicates whether the i-th element belongs to set &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt;), we can use the dot product to tabulate the intersection of two sets &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{B}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[|\mathbf{A} \cap \mathbf{B}| = \sum_{i=1}^na_i \land b_i = \mathbf{a} \cdot \mathbf{b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can calculate this in R by computing on a boolean vector. We compare this to using the dot product.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;14&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;a &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sample&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;, &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;replace =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
b &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sample&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;, &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;replace =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(a &lt;span class=&quot;op&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,]   22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is equivalent to the tabulation provided by the &lt;code&gt;table&lt;/code&gt; function in R. To use the dot product to tally the other frequencies from this table, we just repeat the calculation on the logically negated vectors (i.e., &lt;span class=&quot;math inline&quot;&gt;\(!\mathbf{a}\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(!\mathbf{b}\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;table&lt;/span&gt;(a, b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##        b
## a       FALSE TRUE
##   FALSE    29   32
##   TRUE     17   22&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;TT &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b
TF &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b)
FT &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a) &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b
FF &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a) &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b)
mat &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(FF, TF, FT, TT), &lt;span class=&quot;dt&quot;&gt;nrow =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
mat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]   29   32
## [2,]   17   22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This contingency table (analogous to a confusion matrix) serves as the basis for the Fisher&amp;rsquo;s Exact Test as well as the &lt;span class=&quot;math inline&quot;&gt;\(\chi^2\)&lt;/span&gt; Test. Stated broadly, these test for a significant association between two (or more) presumedly independent sets.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;fisher.test&lt;/span&gt;(mat)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Fisher&amp;#39;s Exact Test for Count Data
## 
## data:  mat
## p-value = 0.8373
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.4850262 2.8537128
## sample estimates:
## odds ratio 
##   1.170924&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;logical-unions&quot; class=&quot;section level3&quot; readability=&quot;15&quot;&gt;
&lt;h3&gt;Logical unions&lt;/h3&gt;
&lt;p&gt;We can also use the dot product to tabulate the frequency of the logical unions between two vectors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[|\mathbf{A} \cup \mathbf{B}| = \sum_{i=1}^na_i \lor b_i = \mathbf{a} \cdot \mathbf{a} + \mathbf{b} \cdot \mathbf{b} - \mathbf{a} \cdot \mathbf{b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can calculate this in R by computing on a boolean vector. We compare this to using the dot product.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(a &lt;span class=&quot;op&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;a &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;a &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;b&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,]   71&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;overlap-coefficient&quot; class=&quot;section level3&quot; readability=&quot;29&quot;&gt;
&lt;h3&gt;Overlap coefficient&lt;/h3&gt;
&lt;p&gt;The dot product also appears in the lesser known, but still important, overlap coefficient. This metric measures the degree to which two sets overlap. The overlap coefficient equation plays a role in network analysis where each &amp;ldquo;set&amp;rdquo; represents the connectivity of one element (called a &lt;em&gt;node&lt;/em&gt;) to all other elements. This provides a way to quantify the amount of common &amp;ldquo;links&amp;rdquo; between any two nodes in a network (Ravasz 2002).&lt;/p&gt;
&lt;p&gt;The overlap coefficient is useful when analyzing biological networks because distinct biological features (e.g., genes) might have similar functional roles if they share a large number of overlapping partners (even if two do not interact directly). A high overlap coefficient means that the two nodes belong to the same &amp;ldquo;neighborhood&amp;rdquo;, regardless of whether the nodes are &amp;ldquo;neighbors&amp;rdquo; themselves (Yip 2007). For two binary vectors, the overlap coefficient equals the frequency of the logical intersections as &amp;ldquo;scaled&amp;rdquo; by the maximum possible number of intersections:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\textrm{overlap}(\mathbf{A},\mathbf{B}) = \frac{|\mathbf{A} \cap \mathbf{B}|}{\textrm{min}(|\mathbf{A}|,|\mathbf{B}|)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the notation &lt;span class=&quot;math inline&quot;&gt;\(|\mathbf{A}| = \sqrt{\mathbf{A} \cdot \mathbf{A}}\)&lt;/span&gt; (yet another &lt;em&gt;norm&lt;/em&gt;) denotes the sum of the absolute value of the elements in the vector.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;inner-product&quot; class=&quot;section level3&quot; readability=&quot;95.5&quot;&gt;
&lt;h3&gt;Inner product&lt;/h3&gt;
&lt;p&gt;Finally, I want to mention that although we discussed the dot product as it pertains to vectors, this operation applies to matrices as well. The dot product of two matrices, called the &lt;strong&gt;inner product&lt;/strong&gt;, is defined as the dot product of the i-th &lt;strong&gt;row&lt;/strong&gt; vector of the first matrix and the j-th &lt;strong&gt;column&lt;/strong&gt; vector of the second matrix (for each row of the first matrix and each column of the second matrix). This means that, unlike the vector dot product, the matrix inner product is not commutative: &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\cdot\mathbf{B}\neq\mathbf{B}\cdot\mathbf{A}\)&lt;/span&gt;. Its definition also requires the first matrix to have the same number of columns as the second has rows.&lt;/p&gt;
&lt;p&gt;Given a matrix &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt; with &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; columns and a matrix &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{B}\)&lt;/span&gt; with &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; rows, the i,j-th result of the inner product equals the dot product of the i-th row of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{A}\)&lt;/span&gt; and the j-th column of &lt;span class=&quot;math inline&quot;&gt;\(\mathbf{B}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[(\mathbf{A}\mathbf{B})_{i,j} = \mathbf{A}_{i,} \cdot \mathbf{B}_{, j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This might look less cryptic in R code. Note that we use the same &lt;code&gt;%*%&lt;/code&gt; operator for matrix multiplication.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;12&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;A &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;)
B &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;rnorm&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;), &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;)
(A &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;B)[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.05027494&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;A[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, ] &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;B[, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,] -0.05027494&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, we will show how to use the inner product to repeat a calculation across the combination of every column vector. Specifically, we will calculate all covariances for a matrix. This is akin to what is achieved by the &lt;code&gt;cov&lt;/code&gt; function in R.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  3.0528358  2.3816630 -1.2354287  0.3243693  0.5110176
## [2,]  2.3816630  2.1267862 -0.5191054  0.2825757  0.2067718
## [3,] -1.2354287 -0.5191054  1.3062685 -0.1832324 -0.7521320
## [4,]  0.3243693  0.2825757 -0.1832324  0.2055409  0.2846195
## [5,]  0.5110176  0.2067718 -0.7521320  0.2846195  1.1981719&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To calculate covariance, we first need to &amp;ldquo;scale&amp;rdquo; each column by the column mean. We can do this using the &lt;code&gt;apply&lt;/code&gt; function. Next, since the inner product computes &lt;strong&gt;rows&lt;/strong&gt; by &lt;strong&gt;columns&lt;/strong&gt;, and we want to compute &lt;strong&gt;columns&lt;/strong&gt; by &lt;strong&gt;columns&lt;/strong&gt;, we need to transpose the first matrix (i.e., in order to have the rows contain column data). Then, we use the &lt;code&gt;%*%&lt;/code&gt; operator to compute the covariance matrix.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;9&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;As &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;apply&lt;/span&gt;(A, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(x) x &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(x))
&lt;span class=&quot;kw&quot;&gt;t&lt;/span&gt;(As) &lt;span class=&quot;op&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;As &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nrow&lt;/span&gt;(As) &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  3.0528358  2.3816630 -1.2354287  0.3243693  0.5110176
## [2,]  2.3816630  2.1267862 -0.5191054  0.2825757  0.2067718
## [3,] -1.2354287 -0.5191054  1.3062685 -0.1832324 -0.7521320
## [4,]  0.3243693  0.2825757 -0.1832324  0.2055409  0.2846195
## [5,]  0.5110176  0.2067718 -0.7521320  0.2846195  1.1981719&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that using the dot product in R is much faster than nested &lt;code&gt;for&lt;/code&gt; loops. In fact, the performance gain from the dot product can hold true even for low level languages like C++ if using a highly optimized linear algebra library (e.g., see &lt;code&gt;RcppEigen&lt;/code&gt;). In cases where a specific function like &lt;code&gt;cov&lt;/code&gt; is not available, dot products can make code run faster and look neater. Nevertheless, thinking in terms of the dot product offers a useful way to unify seemingly disparate statistical concepts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;references&quot; class=&quot;section level3&quot;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ol readability=&quot;4.5294117647059&quot;&gt;
&lt;li readability=&quot;6.71875&quot;&gt;&lt;p&gt;Ravasz, E., A. L. Somera, D. A. Mongru, Z. N. Oltvai, and A.-L. Barab&amp;aacute;si. &amp;ldquo;Hierarchical Organization of Modularity in Metabolic Networks.&amp;rdquo; Science 297, no. 5586 (August 30, 2002): 1551&amp;ndash;55. &lt;a href=&quot;http://dx.doi.org/10.1126/science.1073374&quot; class=&quot;uri&quot;&gt;http://dx.doi.org/10.1126/science.1073374&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li readability=&quot;2.4059405940594&quot;&gt;&lt;p&gt;Yip, Andy M., and Steve Horvath. &amp;ldquo;Gene Network Interconnectedness and the Generalized Topological Overlap Measure.&amp;rdquo; BMC Bioinformatics 8 (2007): 22. &lt;a href=&quot;http://dx.doi.org/10.1186/1471-2105-8-22&quot; class=&quot;uri&quot;&gt;http://dx.doi.org/10.1186/1471-2105-8-22&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;





&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blockads.fivefilters.org&quot;&gt;Let's block ads!&lt;/a&gt;&lt;/strong&gt; &lt;a href=&quot;https://blockads.fivefilters.org/acceptable.html&quot;&gt;(Why?)&lt;/a&gt;&lt;/p&gt;</description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tpq.github.io/the-ubiquitous-dot-product.html</dc:identifier>
</item>
<item>
<title>Genetic Monkeys on Typewriters</title>
<link>http://www.tpq.me/genetic-monkeys-on-typewriters.html</link>
<guid isPermaLink="true" >http://www.tpq.me/genetic-monkeys-on-typewriters.html</guid>
<description>&lt;!-- tabsets --&gt;


&lt;!-- code folding --&gt;






&lt;!--/.navbar --&gt;




&lt;div id=&quot;random-rules&quot; class=&quot;section level3&quot; readability=&quot;19.541062801932&quot;&gt;
&lt;h3&gt;Random rules&lt;/h3&gt;
&lt;p&gt;If a finite number of monkeys can write &lt;a href=&quot;https://en.wikipedia.org/wiki/Infinite_monkey_theorem&quot;&gt;Shakespeare&lt;/a&gt; in infinite time, then an infinite number of monkeys can probably write Shakespeare in finite time. But can a finite number of monkeys write Shakespeare in finite time?&lt;/p&gt;
&lt;p&gt;Tentatively, yes. See, randomness solves problems. How? You guessed it: with enough guesses, you can guess anything right. Trying every possible solution is a well known approach to finding the correct solution (take &lt;a href=&quot;https://en.wikipedia.org/wiki/Bogosort&quot;&gt;bogosort&lt;/a&gt; for example). But guessing takes time and since we never seem to have enough of that, we usually want the right answer right away. The problem is that solving a problem is hard when you do not know much about the problem you are trying to solve. So what happens when you do not know how to implement the optimal solution, but to try &lt;em&gt;every&lt;/em&gt; solution would take too long?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;the-fittest-algorithm&quot; class=&quot;section level3&quot; readability=&quot;50&quot;&gt;
&lt;h3&gt;The fittest algorithm&lt;/h3&gt;
&lt;p&gt;Nature has discovered its own powerful optimization algorithm that successfully balances trial-and-error with speed: &lt;em&gt;the process of evolution by natural selection&lt;/em&gt;. If you can conceive of genes as an attempted &amp;ldquo;solution&amp;rdquo; to the &amp;ldquo;problem&amp;rdquo; of survival until reproduction, then every organism is a guess as to the answer for life.&lt;/p&gt;
&lt;p&gt;From this perspective, genomes that lead to &lt;em&gt;reproduction&lt;/em&gt; represent solutions that work well. Likewise, genomes that lead to &lt;em&gt;reproductive offspring&lt;/em&gt; represent solutions that work even better. Meanwhile, the best solutions would lead to &lt;em&gt;perpetual reproduction&lt;/em&gt;. It would seem then that the &lt;em&gt;Archae&lt;/em&gt;, microscopic organisms with genomes that have remained nearly constant for billions of years, represent a nearly perfect solution to life up until now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Genetic algorithms&lt;/strong&gt; borrow from nature to provide a framework for solving problems through evolutionary discovery. In simple terms, these algorithms offer a construct for iteratively testing the quality of randomly generated solutions, whereby each subsequent generation of solutions derive partially from the best performing solutions of the previous generation. It achieves this by simultaneously exploiting two core principals of natural evolution: (1) mutation and (2) sexual reproduction.&lt;/p&gt;
&lt;p&gt;When an organism reproduces, the offspring incurs random mutations to their inherited genome. Then, through the process of evolution by natural selection, the fittest of these mutations get passed along to the next generation, allowing the most successful prior &amp;ldquo;solutions&amp;rdquo; to seed the next solution set. Some organisms have an additional layer of randomization: &lt;em&gt;recombination&lt;/em&gt;. During this stage of reproduction (colloquially known as sex), the genomes of two parent organisms &lt;em&gt;recombine&lt;/em&gt; with one another through a process called &lt;em&gt;crossing over&lt;/em&gt;. Specifically, the genes of one (presumably successful) parent intermingles with the genes of another (presumably successful) parent, allowing an opportunity for the best (or the worst) traits from each parent to join together.&lt;/p&gt;
&lt;p&gt;(Note that on an individual basis, &lt;em&gt;crossing-over&lt;/em&gt; does not guarantee more successful offspring. However, it appears to have some kind of longitudinal benefit as evidenced by the sheer number of complex organisms that engage in sexual reproduction. In fact, some hermaphroditic organisms, notably flowering plants, will even cross-over with themselves in what is called &lt;em&gt;autogamy&lt;/em&gt;.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;digital-genes&quot; class=&quot;section level3&quot; readability=&quot;42.100814026299&quot;&gt;
&lt;h3&gt;Digital genes&lt;/h3&gt;
&lt;p&gt;Of course, genetic algorithms cannot capture the total complexity of biological systems. As such, &lt;em&gt;digital genes&lt;/em&gt; differ somewhat from their biological counterpart. First, unlike biological genes, which store information as &lt;code&gt;A&lt;/code&gt;s, &lt;code&gt;G&lt;/code&gt;s, &lt;code&gt;T&lt;/code&gt;s, and &lt;code&gt;C&lt;/code&gt;s, digital genes store bits of information as &lt;code&gt;1&lt;/code&gt;s and &lt;code&gt;0&lt;/code&gt;s. Second, unlike biological genes which can grow and shrink from insertions and deletions, digital genes have a fixed size. Third, unlike biological genes which have layers upon layers of higher-order regulation, digital genes have only the simplest expression profiles.&lt;/p&gt;
&lt;p&gt;On the other hand, like biological genes, digital genes require a key that translates the gene transcripts (comprised of evenly spaced &amp;ldquo;codons&amp;rdquo;) into a functional sequence (comprised of protein-like &amp;ldquo;building blocks&amp;rdquo;). This key, analogous to the &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_codon_table&quot;&gt;DNA codon table&lt;/a&gt;, is a user-defined function that ultimately tells the genetic algorithm how to make sense of (pseudo-)random binary strings.&lt;/p&gt;
&lt;p&gt;To demonstrate a basic &amp;ldquo;codon table&amp;rdquo;, I introduce a function below that translates a &lt;em&gt;binary sequence&lt;/em&gt; (analogous to a genetic codon) into a &lt;em&gt;decimal sequence&lt;/em&gt; (analogous to an amino acid). Amino acids, the &amp;ldquo;building blocks&amp;rdquo; of the cell, conventionally represent the functional level upon which natural selection works. Then, just as amino acids undergo the test of life to determine the fitness of that organism, these translated decimal sequences will undergo their own test to determine their &amp;ldquo;fitness&amp;rdquo; as a solution.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;8&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;table &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(codon){
  
  &lt;span class=&quot;kw&quot;&gt;strtoi&lt;/span&gt;(codon, &lt;span class=&quot;dt&quot;&gt;base =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;digital-ribosomes&quot; class=&quot;section level3&quot; readability=&quot;42.5&quot;&gt;
&lt;h3&gt;Digital ribosomes&lt;/h3&gt;
&lt;p&gt;In nature, each codon consists of three bases which exist in one of four states. Since we can represent four states using two bits, each natural codon contains &lt;span class=&quot;math inline&quot;&gt;\(3(2)\)&lt;/span&gt; bits of information. We will follow this convention for now by using 6-bit &lt;em&gt;digital codons&lt;/em&gt;, allowing for &lt;span class=&quot;math inline&quot;&gt;\(2^6\)&lt;/span&gt; values per codon. An example of a digital gene with two 6-bit codons might look like this:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;18&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;twoCodons &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Above, I introduced a function that translates a single &lt;em&gt;digital codon&lt;/em&gt; into a decimal sequence. However, in order to perform real work, we need to harness the power of multiple codons by translating an entire &lt;em&gt;digital genome&lt;/em&gt;. Within the cell, the &lt;strong&gt;ribosome&lt;/strong&gt; organelle holds the machinery that translates genes into functional proteins. To reinforce analogy, we will embed our custom &amp;ldquo;codon table&amp;rdquo; within a &lt;em&gt;digital ribosome&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;17&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ribosome &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(string, &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;how =&lt;/span&gt; table){
  
  &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt;(&lt;span class=&quot;op&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;is.character&lt;/span&gt;(string)){
    
    string &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(string, &lt;span class=&quot;dt&quot;&gt;collapse =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;)
  }
  
  &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(string) &lt;span class=&quot;op&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;){
    
    &lt;span class=&quot;kw&quot;&gt;stop&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Digital gene not evenly divisible by chosen codon size.&amp;quot;&lt;/span&gt;)
  }
  
  output &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;vector&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;list&amp;quot;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(string)&lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;nbits)
  &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt;(i &lt;span class=&quot;cf&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(string)&lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)){
    
    codon &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;substr&lt;/span&gt;(string, i&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, i&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;nbits &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;nbits)
    output[[i&lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;do.call&lt;/span&gt;(how, &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(codon))
  }
  
  &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt;(output)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, by feeding the mock &lt;em&gt;digital gene&lt;/em&gt; through this &lt;em&gt;digital ribosome,&lt;/em&gt; we can convert a series of evenly spaced binary strings into their corresponding decimal values.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(twoCodons)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 41
## 
## [[2]]
## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;digital-fitness&quot; class=&quot;section level3&quot; readability=&quot;58.5&quot;&gt;
&lt;h3&gt;Digital fitness&lt;/h3&gt;
&lt;p&gt;Now, we need to put the translated gene to the test. For this, we define a &lt;em&gt;fitness function&lt;/em&gt; that will score the &amp;ldquo;fitness&amp;rdquo; of a digital genome. In this paradigm, I find it helpful to think of &amp;ldquo;fitness&amp;rdquo; as a measure of the proximity of an outcome to an intended result. By convention, the fitness function will convert a binary vector input into a &lt;em&gt;survival score&lt;/em&gt;, such that higher scores signify greater fitness. Then, through simulated evolution, the more fit solutions (i.e., those with higher scores) will have an increased probability of seeding the next generation of solutions.&lt;/p&gt;
&lt;p&gt;In the example below, we define a fitness function that solves the simple equation &lt;span class=&quot;math inline&quot;&gt;\(a^2+b=308\)&lt;/span&gt;. Since the fitness function expects binary vector input, we embed our &lt;em&gt;digital ribosome&lt;/em&gt; within the fitness function itself. Then, based on our choice of genome size and ribosome configuration, the genetic algorithm will attempt solutions for &lt;span class=&quot;math inline&quot;&gt;\(0&amp;lt;a&amp;lt;65\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(0&amp;lt;b&amp;lt;65\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(Note that if we wanted to change the range of values for &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(b\)&lt;/span&gt;, we just need to change the &amp;ldquo;codon table&amp;rdquo; function. If we wanted to add more possible values for &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(b\)&lt;/span&gt;, we just need to increase the size of the codon. If we wanted to add more variables, we just need to increase the size of the genome. In this way, we have a tremendous amount of control over the kinds of solutions tested. Meanwhile, through careful crafting of the fitness function, we maintain total control over how the solutions get tested.)&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;10&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;fitness &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(binary){
  
  AA &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(binary, &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;how =&lt;/span&gt; table)
  solution &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;AA[[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]]&lt;span class=&quot;op&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;AA[[&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]]
  &lt;span class=&quot;kw&quot;&gt;atan&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;abs&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;308&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;solution))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In order to execute the algorithm, we need to provide two additional parameters. The first, &lt;code&gt;nBits&lt;/code&gt;, equals the total length of the &lt;em&gt;digital genome&lt;/em&gt; expected by the &lt;em&gt;digital ribosome&lt;/em&gt; and &lt;em&gt;fitness function&lt;/em&gt;. The second, &lt;code&gt;maxiter&lt;/code&gt;, equals the total number of generations (i.e., reproductive cycles) until the algorithm terminates. Running an algorithm with fewer iterations will finish sooner, but will carry an increased risk of not converging onto a &amp;ldquo;best&amp;rdquo; solution.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;11&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(GA)
&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)
res &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ga&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fitness =&lt;/span&gt; fitness, &lt;span class=&quot;dt&quot;&gt;nBits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;maxiter =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://tpq.github.io/genetic-monkeys-on-typewriters_files/figure-html/unnamed-chunk-7-1.png&quot; width=&quot;672&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;solution &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;summary&lt;/span&gt;(res)&lt;span class=&quot;op&quot;&gt;$&lt;/span&gt;solution
&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(solution)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 17
## 
## [[2]]
## [1] 19&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;more-than-one-answer&quot; class=&quot;section level3&quot; readability=&quot;18&quot;&gt;
&lt;h3&gt;More than one answer&lt;/h3&gt;
&lt;p&gt;This mock example highlights how the same genetic algorithm, when executed twice, may arrive at different solutions each time. By starting with a different genetic stock (i.e., a different primordial Eve), or by drifting through a different genetic path, a genetic algorithm may converge on a different &amp;ldquo;best&amp;rdquo; solution.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;11&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
res &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ga&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fitness =&lt;/span&gt; fitness, &lt;span class=&quot;dt&quot;&gt;nBits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;maxiter =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;summary&lt;/span&gt;(res)&lt;span class=&quot;op&quot;&gt;$&lt;/span&gt;solution)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;literary-genes&quot; class=&quot;section level3&quot; readability=&quot;78.5&quot;&gt;
&lt;h3&gt;Literary genes&lt;/h3&gt;
&lt;p&gt;Since we can represent nearly any type of input as a binary string, genetic algorithms offer a tremendous amount of flexibility when solving problems. Next, we will use genetic algorithms to draft a line from Shakespeare. By constructing a fitness function that rewards digital genes that most closely approximate Shakespearean verse, we guide an otherwise chaotic process toward coherency. Then, just like the proverbial monkey striking away on a typewriter, we author something poetically sensible out of nothing more than fragments of nonsense.&lt;/p&gt;
&lt;p&gt;First, however, we need a new &amp;ldquo;codon table&amp;rdquo; that translates binary strings to alphanumeric characters. To do this, we will convert &lt;em&gt;binary&lt;/em&gt; to &lt;em&gt;decimal&lt;/em&gt;, then &lt;em&gt;decimal&lt;/em&gt; to &lt;em&gt;raw&lt;/em&gt;, and finally &lt;em&gt;raw&lt;/em&gt; to &lt;em&gt;character&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;8&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;table &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(codon){
  
  dec &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;strtoi&lt;/span&gt;(codon, &lt;span class=&quot;dt&quot;&gt;base =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
  &lt;span class=&quot;kw&quot;&gt;rawToChar&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;as.raw&lt;/span&gt;(dec))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By using this new &amp;ldquo;codon table&amp;rdquo; in conjunction with the digital ribosome from above, we can convert a digital genome into a literary paragraph. However, in order to ensure that each codon could eventually become any letter, we need these codons to contain 7-bits of information each.&lt;/p&gt;
&lt;p&gt;Next, we need a way to evaluate the fitness, or literary quality, of the translated binary strings. In theory, an experienced user could incorporate some kind of natural language processing to guide the algorithm toward novel speech. However, in this contrived example, we will simply excerpt a quote from Shakespeare against which we compare each piece of translated text. As such, our fitness function will measure the similarity between Shakespeare and random nonsense. Then, by iteratively breeding the most sensible nonsense, we can eventually arrive at some prosaic truth.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;7&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;prose &amp;lt;-&lt;span class=&quot;st&quot;&gt; &amp;quot;that which we call a rose by any other name would smell as sweet&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;11&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;fitness &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cf&quot;&gt;function&lt;/span&gt;(binary){
  
  AA &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(binary, &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;how =&lt;/span&gt; table)
  &lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;unlist&lt;/span&gt;(AA) &lt;span class=&quot;op&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;unlist&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;strsplit&lt;/span&gt;(prose, &lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;)))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In order to execute the algorithm, we again need to supply arguments for the &lt;code&gt;nBits&lt;/code&gt; and &lt;code&gt;maxiter&lt;/code&gt; parameters. Since we need 7-bits to represent each letter in the final result, we set &lt;code&gt;nBits&lt;/code&gt; equal to &lt;code&gt;nchar(prose)*7&lt;/code&gt;. Next, we set &lt;code&gt;maxiter&lt;/code&gt; to 100,000 generations, a value which led to promising results during prior testing.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;11&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)
res &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ga&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fitness =&lt;/span&gt; fitness, &lt;span class=&quot;dt&quot;&gt;nBits =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;nchar&lt;/span&gt;(prose)&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;maxiter =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;100000&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Sometimes, the genetic algorithms result object contains &lt;em&gt;multiple&lt;/em&gt; equally fit solutions. In these cases, I usually collapse all &amp;ldquo;best&amp;rdquo; solutions into an &amp;ldquo;average best&amp;rdquo; solution.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;9&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;allsol &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;summary&lt;/span&gt;(res)&lt;span class=&quot;op&quot;&gt;$&lt;/span&gt;solution
&lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nrow&lt;/span&gt;(allsol) &lt;span class=&quot;op&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;){
  
  final &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;round&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;colSums&lt;/span&gt;(allsol) &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nrow&lt;/span&gt;(allsol))
}&lt;span class=&quot;cf&quot;&gt;else&lt;/span&gt;{
  
  final &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;allsol[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, ]
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can join the &amp;ldquo;best&amp;rdquo; characters together to draft the final sentence.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; readability=&quot;9&quot;&gt;&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;paste&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;ribosome&lt;/span&gt;(final, &lt;span class=&quot;dt&quot;&gt;nbits =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;7&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;collapse =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;that which we call a rose by any other name would smell as sweet&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&quot;afterward&quot; class=&quot;section level3&quot; readability=&quot;26.96275862069&quot;&gt;
&lt;h3&gt;Afterward&lt;/h3&gt;
&lt;p&gt;Finally, I should remind the reader that the monkey-on-typewriter proverb does not accurately portray the simian creative process. Monkeys do not operate as random text generators, but rather have their own hopes, dreams, goals, desires, and even their own aesthetic. In fact, researchers have put monkeys to the test, providing them with typewriters, food, and time (the raw materials of fiction), and then waited for &lt;em&gt;Hamlet&lt;/em&gt; to happen. However, as noted by a prestigious journalist, &amp;ldquo;after a month, the Sulawesi crested macaques had only succeeded in partially destroying the machine, using it as a lavatory, and mostly typing the letter &lt;a href=&quot;http://news.bbc.co.uk/2/hi/3013959.stm&quot;&gt;s&lt;/a&gt;,&amp;rdquo; which is about all I can muster together myself most months.&lt;/p&gt;

&lt;/div&gt;





&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blockads.fivefilters.org&quot;&gt;Let's block ads!&lt;/a&gt;&lt;/strong&gt; &lt;a href=&quot;https://blockads.fivefilters.org/acceptable.html&quot;&gt;(Why?)&lt;/a&gt;&lt;/p&gt;</description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tpq.github.io/genetic-monkeys-on-typewriters.html</dc:identifier>
</item>
</channel>
</rss>
